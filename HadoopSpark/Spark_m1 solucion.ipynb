{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# La colecci\u00f3n del MoMA (Museo de Arte Moderno) de Nueva York\n\n### Disponible en Kaggle en:\nhttps://www.kaggle.com/momanyc/museum-collection\n\n(no obstante, debes usar exclusivamente el dataset que has recibido adjunto al email)"}, {"cell_type": "markdown", "metadata": {}, "source": "\n### Contexto\n\nEl Museo de Arte Moderno (MoMA) adquiri\u00f3 sus primeras obras de arte en 1929, a\u00f1o de su fundaci\u00f3n. Hoy, la colecci\u00f3n en evoluci\u00f3n del Museo contiene casi 200.000 obras de todo el mundo que abarcan los \u00faltimos 150 a\u00f1os. La colecci\u00f3n incluye una gama cada vez mayor de expresi\u00f3n visual, que incluye pintura, escultura, grabado, dibujo, fotograf\u00eda, arquitectura, dise\u00f1o, cine y medios y artes esc\u00e9nicas.\n\n### Contenido\n\nMoMA se compromete a ayudar a todos a comprender, disfrutar y utilizar nuestra colecci\u00f3n. El sitio web del Museo presenta 72.706 obras de arte de 20.956 artistas. El conjunto de datos de obras de arte contiene 130.262 registros, que representan todas las obras que se han incorporado a la colecci\u00f3n del MoMA y est\u00e1n catalogadas en nuestra base de datos. Incluye metadatos b\u00e1sicos para cada obra, incluyendo t\u00edtulo, artista, fecha, medio, dimensiones y fecha de adquisici\u00f3n por parte del Museo. Algunos de estos registros tienen informaci\u00f3n incompleta y se indican como \"no aprobados por el curador\". El conjunto de datos de artistas contiene 15.091 registros, que representan a todos los artistas que tienen obras en la colecci\u00f3n del MoMA y han sido catalogados en nuestra base de datos. Incluye metadatos b\u00e1sicos para cada artista, incluido el nombre, la nacionalidad, el sexo, el a\u00f1o de nacimiento y el a\u00f1o de fallecimiento.\n\n\n### Variables y significado (s\u00f3lo aquellas que se utilizar\u00e1n)\n\n\n* Title: string - t\u00edtulo de la obra\n* Artist: string - nombre del autor o autores\n* ConstituentID: string - identificador no usado\n* ArtistBio: string - biograf\u00eda del autor\n* Nationality: string - nacionalidad del autor(es)\n* BeginDate: string - fecha(s) de nacimiento del autor(es)\n* EndDate: string - fecha(s) de fallecimiento del autor(es)\n* Gender: string - g\u00e9nero del autor(es)\n* Date: string - fecha de creaci\u00f3n de la obra\n* Medium: string - soporte f\u00edsico en el que se cre\u00f3 la obra\n* Dimensions: string - dimensiones (todas)\n* CreditLine: string - forma de obtenci\u00f3n de esa obra de arte por parte del museo\n* AccessionNumber: string - identificador no usado\n* Classification: string - tipolog\u00eda de la obra de arte\n* Department: string - departamento del museo al que pertenece\n* DateAcquired: string - fecha de adquisici\u00f3n\n* Cataloged: string - si est\u00e1 catalogada o no\n* ObjectID: string - identificador no usado\n* URL: string - enlace a la web del museo\n* ThumbnailURL: string - enlace a una imagen en miniatura\n* Circumference (cm): string - per\u00edmetro de la obra en cm\n* Depth (cm): string - profundidad en cm\n* Diameter (cm): string - di\u00e1metro en cm\n* Height (cm): string - altura en cm\n* Length (cm): string - longitud en cm\n* Weight (kg): string - peso de la obra en kg\n* Width (cm): string - anchura de la obra en cm\n* Seat Height (cm): string - altura del soporte donde se expone\n* Duration (sec.): string - duraci\u00f3n de la obra en caso de ser audiovisual\n"}, {"cell_type": "markdown", "metadata": {}, "source": "**Nombre completo del alumno: Bruno Urban Alfaro"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d664812d734466d986ae91e421e0c696", "grade": false, "grade_id": "cell-0afbc8d5ce9eb796", "locked": true, "schema_version": 3, "solution": false, "task": false}, "tags": []}, "source": "**INSTRUCCIONES**: en cada celda debes responder a la pregunta formulada, asegur\u00e1ndote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a None. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del c\u00e1lculo quede guardado exactamente en la variable que ven\u00eda inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). \n\n**No olvides borrar la l\u00ednea raise NotImplementedError() de cada celda cuando hayas completado la soluci\u00f3n de esa celda y quieras probarla.**\n\nDespu\u00e9s de cada celda evaluable ver\u00e1s una celda con c\u00f3digo. Ejec\u00fatala (no modifiques su c\u00f3digo) y te dir\u00e1 si tu soluci\u00f3n es correcta o no. Adem\u00e1s de esas pruebas, se realizar\u00e1n algunas m\u00e1s (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la soluci\u00f3n correcta o no. Aseg\u00farate de que, al menos, todas las celdas indican que el c\u00f3digo es correcto antes de enviar el notebook terminado.\n\n**Nunca se debe redondear ninguna cantidad si no lo pide expl\u00edcitamente el enunciado**"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d22d622bad7f7572f4c1d832d4a15f35", "grade": false, "grade_id": "cell-c101269ae661fc72", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "## Sobre el dataset anterior (MoMA_Artworks.csv) se pide:"}, {"cell_type": "markdown", "metadata": {}, "source": "**(1 punto)** Ejercicio 1\n\n* Leerlo tratando de que Spark infiera el tipo de dato de cada columna, y **cachearlo**. Debe guardarse en una variable llamada `artworks`.\n* Puesto que existen columnas que contienen una coma enmedio del valor, en esos casos los valores vienen entre comillas dobles. Spark ya contempla esta posibilidad y puede leerlas adecuadamente **si al leer le indicamos las siguientes opciones adicionales** adem\u00e1s de las que ya sueles usar: `.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")`.\n* Aseg\u00farate de que las **filas que no tienen el formato correcto sean descartadas**, indicando tambi\u00e9n la opci\u00f3n `mode` con el valor `DROPMALFORMED` como vimos en clase."}, {"cell_type": "code", "execution_count": 1, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "ebccd2b2f8af6e1228e1f9c503932f12", "grade": false, "grade_id": "read_csv", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:=============================>                             (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- Title: string (nullable = true)\n |-- Artist: string (nullable = true)\n |-- ConstituentID: string (nullable = true)\n |-- ArtistBio: string (nullable = true)\n |-- Nationality: string (nullable = true)\n |-- BeginDate: string (nullable = true)\n |-- EndDate: string (nullable = true)\n |-- Gender: string (nullable = true)\n |-- Date: string (nullable = true)\n |-- Medium: string (nullable = true)\n |-- Dimensions: string (nullable = true)\n |-- CreditLine: string (nullable = true)\n |-- AccessionNumber: string (nullable = true)\n |-- Classification: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- DateAcquired: string (nullable = true)\n |-- Cataloged: string (nullable = true)\n |-- ObjectID: string (nullable = true)\n |-- URL: string (nullable = true)\n |-- ThumbnailURL: string (nullable = true)\n |-- Circumference (cm): string (nullable = true)\n |-- Depth (cm): string (nullable = true)\n |-- Diameter (cm): string (nullable = true)\n |-- Height (cm): string (nullable = true)\n |-- Length (cm): string (nullable = true)\n |-- Weight (kg): string (nullable = true)\n |-- Width (cm): string (nullable = true)\n |-- Seat Height (cm): string (nullable = true)\n |-- Duration (sec.): string (nullable = true)\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import functions as F\nartworks = spark.read\\\n                 .option(\"header\", \"true\")\\\n                 .option(\"inferSchema\", \"true\")\\\n                 .option(\"quote\", \"\\\"\")\\\n                 .option(\"escape\", \"\\\"\")\\\n                 .option(\"mode\", \"DROPMALFORMED\")\\\n                .csv(\"gs://tarea--master/data/MoMA_Artworks.csv\") \n# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n\nartworks.printSchema()"}, {"cell_type": "code", "execution_count": 2, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "ebccd2b2f8af6e1228e1f9c503932f12", "grade": false, "grade_id": "read_csv", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "23/06/24 17:03:33 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"}, {"data": {"text/plain": "DataFrame[Title: string, Artist: string, ConstituentID: string, ArtistBio: string, Nationality: string, BeginDate: string, EndDate: string, Gender: string, Date: string, Medium: string, Dimensions: string, CreditLine: string, AccessionNumber: string, Classification: string, Department: string, DateAcquired: string, Cataloged: string, ObjectID: string, URL: string, ThumbnailURL: string, Circumference (cm): string, Depth (cm): string, Diameter (cm): string, Height (cm): string, Length (cm): string, Weight (kg): string, Width (cm): string, Seat Height (cm): string, Duration (sec.): string]"}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": "# Cachear el DataFrame\n#nos aseguramos de que al preguntar si esta cacheado sale true una vez cacheamos\n \nartworks.cache()\n#artworks.is_cached  "}, {"cell_type": "code", "execution_count": 3, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "cbe721b16a3c35e82b57ec6c28b04fbb", "grade": true, "grade_id": "read_csv_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "assert(artworks.count() == 128234)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "f9fd60af913dde963f5500f9579b4c94", "grade": false, "grade_id": "cell-11065b99ddfcbb80", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(1 punto)** Ejercicio 2\n\nPuesto que vamos a hacer c\u00e1lculos relativos a las fechas de nacimiento y muerte de los autores de las obras, es necesario tenerlas como n\u00famero. Actualmente est\u00e1n como a\u00f1os entre par\u00e9ntesis, ej. (1924) o a veces como varios par\u00e9ntesis, \ncomo \"(1924) (1931) (1918)\" cuando son varios autores. Vamos a limpiar esto para obtener en BeginDate y en EndDate una columna num\u00e9rica.\n\nPartiendo del DF almacenado en la variable `artworks`, haciendo una \u00fanica secuencia de transformaciones encadenadas, se pide:\n\n* Quedarse s\u00f3lo con aquellas obras en las que `BeginDate` no es null y `EndDate` tampoco.\n* Sobre el DF resultante, reemplazar la columna `BeginDate` por el resultado de quitar los par\u00e9ntesis. Consulta la documentaci\u00f3n de la funci\u00f3n `F.regexp_replace(nombreCol, stringBuscar, stringReemplazo)` donde el segundo argumento debe ser `\"\\(|\\)\"` que indica buscar tanto el ( como el ), y el tercer argumento debe ser la cadena vac\u00eda `\"\"` para que los elimine.\n* **Despu\u00e9s de hacer esto**, la columna `BeginDate` debe ser reemplazada de nuevo por una columna de tipo vector, que es el resultado de invocar a `F.split(nombreCol, separador)` (funci\u00f3n de `pyspark.sql.functions` que se aplica a una columna y devuelve otra columna de tipo vector tras haber cortado cada fila de la columna original por el car\u00e1cter separador indicado). En nuestro caso el separador debe ser el car\u00e1cter `\" \"` (un espacio en blanco).\n* F\u00edjate que estos dos pasos se pueden hacer en una sola l\u00ednea, ya que es posible invocar a `F.split(F.regexp_replace(...))`.\n* Repetir los dos pasos anteriores en el mismo orden con la columna `EndDate`.\n* No te preocupes por las columnas que tengan una sola fecha: la funci\u00f3n split las convertir\u00e1 en vectores de un solo elemento.\n* Aplicar `F.split` para reemplazar la columna `Artist` por el resultado de cortar por el string  `\", \"` (es decir, la coma seguida de un espacio en blanco).\n* Por \u00faltimo, crear la coluna `n_autores` como la longitud de cada vector de la columna `Artist`, usando la funci\u00f3n `F.size(nombreCol)` que se aplica a columnas de tipo colecci\u00f3n.\n* El resultado debe guardarse en una variable llamada `obras_splitted_df`."}, {"cell_type": "code", "execution_count": 4, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "fbb261c5b58380921e41d4040b5bfc3e", "grade": false, "grade_id": "split_fechas", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE. NO RENOMBRAR LAS VARIABLES\n#obras_splitted_df = None   # Sustituye None por las operaciones adecuadas\n# YOUR CODE HERE\n\n# Quedarse s\u00f3lo con aquellas obras en las que BeginDate no es null y EndDate tampoco\nobras_filtradas_df = artworks.filter((F.col(\"BeginDate\").isNotNull()) & (F.col(\"EndDate\").isNotNull()))\n\n# Reemplazar la columna BeginDate por el resultado de quitar los par\u00e9ntesis\nobras_sin_parentesis_df = obras_filtradas_df.withColumn(\"BeginDate\", F.split(F.regexp_replace(F.col(\"BeginDate\"), \"\\(|\\)\", \"\"), \" \"))\n\n# Reemplazar la columna EndDate por el resultado de quitar los par\u00e9ntesis\nobras_splitted_df = obras_sin_parentesis_df.withColumn(\"EndDate\", F.split(F.regexp_replace(F.col(\"EndDate\"), \"\\(|\\)\", \"\"), \" \"))\n\n# Aplicar F.split para reemplazar la columna Artist por el resultado de cortar por el string \", \"\nobras_splitted_df = obras_splitted_df.withColumn(\"Artist\", F.split(F.col(\"Artist\"), \", \"))\n\n# Crear la coluna n_autores como la longitud de cada vector de la columna Artist, usando la funci\u00f3n F.size(nombreCol)\nobras_splitted_df = obras_splitted_df.withColumn(\"n_autores\", F.size(F.col(\"Artist\")))"}, {"cell_type": "code", "execution_count": 5, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "410f3c6bbe735f9b0bab3b0e89245adc", "grade": true, "grade_id": "split_fechas_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "assert(obras_splitted_df.count() == 126970)   # n\u00famero de filas tras quitar las que tienen BeginDate y EndDate a null\ntipos = dict(obras_splitted_df.dtypes)\nassert(tipos[\"Artist\"] == \"array<string>\")  # la columna Artist ahora debe ser una columna de vectores de string\nassert(tipos[\"BeginDate\"] == \"array<string>\") # la columna BeginDate ahora debe ser una columna de vectores de string\nassert(tipos[\"EndDate\"] == \"array<string>\") # la columna EndDate debe ahora ser una columna de vectores de string\nassert(tipos[\"n_autores\"] == \"int\") # la columna EndDate debe ahora ser una columna de vectores de string"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "b76ca293cd3ecd7efbb0da34a401400a", "grade": false, "grade_id": "enunciado_todos_enteros", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(2 puntos)** Ejercicio 3\n\nEn este ejercicio vamos a convertir todos los elementos de los vectores en n\u00fameros enteros. Adem\u00e1s, a pesar de c\u00f3mo hemos tratado de separar los elementos, existen algunas filas donde las columnas `BeginDate` y `EndDate` traen un texto que no son n\u00fameros entre par\u00e9ntesis como nos gustar\u00eda, as\u00ed que la separaci\u00f3n no habr\u00e1 funcionado muy bien. Vamos a quedarnos solo con aquellas filas que contienen exclusivamente enteros en las columnas.\n\nPartiendo de `obras_splitted_df` se pide:\n\n* Reemplazar la coluna `BeginDate` por el resultado de aplicar, en cada fila, una funci\u00f3n definida por el usuario a **cada elemento del vector**. Existe en Pyspark una funci\u00f3n llamada `transform` que hace justamente eso, pero en la versi\u00f3n 2.4 todav\u00eda no se puede invocar con la API de columnas sino que *existe solamente en la API de SQL puro* (esto es frecuente cuando los creadores de Spark introducen funciones nuevas: empieza estando disponible s\u00f3lo para SQL puro y no para la API estructurada, que suele llegar en versiones posteriores).\n  * Para poder usarla, escribe `F.expr(\"transform(BeginDate, x -> int(x))\"))` como segundo argumento de `withColumn(...)`, donde la funci\u00f3n que estamos aplicando a cada elemento del vector es simplemente transformarlo a entero.\n* Haz lo mismo con la columna `EndDate`.\n* A continuaci\u00f3n, escribe una UDF que se aplique a una columna de tipo vector y compruebe si todos los elementos son enteros, en cuyo caso debe devolver `True`, y en caso contrario debe devolver `False`. Para ello:\n  * Rellena el esqueleto de la funci\u00f3n de Python `todos_enteros` que tienes esbozada. El argumento que va a recibir siempre ser\u00e1 una lista de Python por la que debes iterar, comprobando si cada elemento es de tipo entero. En el momento en el que encuentres uno que no es entero, ya puedes directamente retornar False. Si el bucle finaliza sin haber retornado en ninguna iteraci\u00f3n, significa que todos los elementos son enteros, as\u00ed que debe devolver True. Utiliza la funci\u00f3n de Python `isinstance(x, int)` para comprobar si un elemento es entero.\n  * Crea en una variable `todos_enteros_udf` el objeto UDF que envuelve a la funci\u00f3n anterior. Recuerda indicar que el valor devuelto por la funci\u00f3n es BooleanType(), el cual debes importar adecuadamente para poder usarlo.\n\nCuando tengas hecha la UDF:\n* Apl\u00edcala dentro de `withColumn(...)` para crear una nueva columna de booleanos llamada `enteros_begin` donde se indique si todos los elementos de cada fila de la columna `BeginDate` son enteros. Haz lo mismo para crear otra columna `enteros_end` que indique si todos los elementos de cada fila de la columna `EndDate` son enteros.\n* **Despu\u00e9s de haber hecho lo anterior**, filtra las filas para quedarte solamente con aquellas en las que la longitud de cada vector de la columna `BeginDate` es igual a la longitud del vector de la columna `Artist` de esa misma fila, y adem\u00e1s la columna `enteros_begin` es True en esa fila y adem\u00e1s la columna `enteros_end` es True en esa fila (condici\u00f3n booleana compuesta por 3 condiciones simples).\n  * Consulta la documentaci\u00f3n de la funci\u00f3n `F.size(...)` que se aplica a una columna de tipo vector y devuelve una columna de enteros que son sus longitudes).\n* Primero debes escribir la UDF, y despu\u00e9s una \u00fanica secuencia de *cinco* transformaciones encadenadas que resuelva todo lo que pide este ejercicio. El resultado debe guardarse en la variable `obras_preprocesado_df`."}, {"cell_type": "code", "execution_count": 6, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "47961f67927f142c02de0b8e4c145f6e", "grade": false, "grade_id": "todos_enteros", "locked": false, "schema_version": 3, "solution": true, "task": false}, "tags": []}, "outputs": [], "source": "\nfrom pyspark.sql.types import BooleanType\n\ndef todos_enteros(lista):\n    for x in lista:\n        if not isinstance(x, int):\n            return False\n    return True\n\ntodos_enteros_udf = F.udf(todos_enteros, BooleanType())\n\nobras_preprocesado_df = obras_splitted_df\\\n    .withColumn(\"BeginDate\", F.expr(\"transform(BeginDate, x -> int(x))\")) \\\n    .withColumn(\"EndDate\", F.expr(\"transform(EndDate, x -> int(x))\")) \\\n    .withColumn(\"enteros_begin\", todos_enteros_udf(\"BeginDate\")) \\\n    .withColumn(\"enteros_end\", todos_enteros_udf(\"EndDate\")) \\\n    .filter((F.size(\"BeginDate\") == F.size(\"Artist\")) &\n            F.col(\"enteros_begin\")  & F.col(\"enteros_end\")) \n"}, {"cell_type": "code", "execution_count": 7, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "3a5dbe0d4cda723805b3cebcf48b074f", "grade": true, "grade_id": "todos_enteros_test", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "assert(obras_preprocesado_df.count() == 126224)\n\nfrom pyspark.sql import types as T\nassert(todos_enteros([4, -123, 0, 23]) and not todos_enteros([23, 12, \"12\"]))  # probamos la funci\u00f3n todos_enteros\nassert(todos_enteros_udf.returnType == T.BooleanType())  # la UDF debe devolver un tipo booleano\n\nr = obras_preprocesado_df.where(\"Title = 'House IV Project, Falls Village, Connecticut (Multiple axonometrics)'\")\\\n.select(\"Title\", \"Artist\", \"BeginDate\", \"EndDate\").first()\nassert(r.Artist == [\"Peter Eisenman\", \"Robert Cole\"] and r.BeginDate == [1932, 0] and r.EndDate == [0, 0])\n\ntipos = dict(obras_preprocesado_df.dtypes)\nassert(tipos[\"BeginDate\"] == \"array<int>\")\nassert(tipos[\"EndDate\"] == \"array<int>\")\nassert(tipos[\"enteros_begin\"] == \"boolean\")\nassert(tipos[\"enteros_end\"] == \"boolean\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "d1c708deb5076dfee439d39d074e5e76", "grade": false, "grade_id": "cell-3cdceb86b7473ad3", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(2 puntos)** Ejercicio 4\n\nYa tenemos como vectores de enteros las columnas `BeginDate`, `EndDate` y `Artist`. Si todo ha ido bien, las longitudes de los vectores de esas tres columnas deben coincidir: si en determinada fila `BeginDate` contiene un vector de longitud 2 (por ejemplo), tambi\u00e9n deber\u00edan tener longitud 2 el vector de la columna `EndDate` de esa misma fila y el de la columna `Artist`de esa misma fila. \n\nAhora queremos **explotar** esas columnas, es decir, si en `BeginDate` hay un vector de **n** elementos, es porque la obra tiene **n** autores distintos, pero nos gustar\u00eda que aparezca como **n filas distintas** en nuestro DF, en lugar de venir comprimido en una \u00fanica fila con vectores. En cada una de esas filas separadas, nos gustar\u00eda ver a un autor concreto de la  obra, con su nombre, su fecha de nacimiento y de fallecimiento. Los valores del resto de columnas ser\u00e1n id\u00e9nticos en esas **n** filas, y s\u00f3lo difieren las columnas Artist, BeginDate y EndDate. \n\n\u00bfPero cu\u00e1l de las tres columnas de tipo vector (`Artist, BeginDate, EndDate`) debemos usar para explotar? Utilizaremos *las tres conjuntamente*.\n\nPartiendo del DF `obras_preprocesado_df` construido en el ejercicio anterior, se pide:\n\n* Crear mediante `withColumn(...)` una nueva columna llamada `tripletas` que en cada fila tenga un **vector de estructuras**, fusionando para ello las columnas `Artist`, `BeginDate` y `EndDate`, invocando a la funci\u00f3n `F.arrys_zip(nombreCol1, nombreCol2, nombreCol3)` que devuelve un objeto Column de tipo vector de estructuras (en nuestro caso, cada estructura ser\u00e1 una tripleta con tres campos llamados `Artist`, `BeginDate`, `EndDate`). \n  * La funci\u00f3n `F.arrays_zip` funciona adecuadamente porque los vectores que fusionamos tienen siempre el mismo tama\u00f1o entre s\u00ed (aunque el tama\u00f1o puede ser distinto en cada fila, pero coincide para esas columnas en cada fila concreta).\n  * Lo que hace la funci\u00f3n es fusionar en una estructura elemento i-\u00e9simo de cada uno de los vectores. La primera estructura estar\u00e1 formada por el primer elemento de Artist, de BeginDate y de EndDate; la segunda estructura ser\u00e1 el segundo elemento de Artist, de BeginDate y de EndDate, y as\u00ed sucesivamente. Todas estas estructuras se almacenan en un vector de estructuras en esa fila en la columna `tripletas`.\n* **Despu\u00e9s de haber hecho esto**, se pide reemplazar la columna `tripletas` por el resultado de **explotar** dicha columna mediante la funci\u00f3n `F.explode(nombreCol)`. El resultado ser\u00e1 una columna de tripletas, donde ya no hay vectores puesto que cada vector de tripletas ha sido explotado, dando lugar a **varias filas independientes, en las cuales hay una tripleta distinta (Artist, BeginDate, EndDate) del vector explotado**. En el resto de columnas todas las filas son id\u00e9nticas entre s\u00ed.\n* Por \u00faltimo, vamos a extraer como columnas cada uno de los campos de las estructuras. Se pide:\n  * Reemplazar la columna `BeginDate` por el campo `tripletas.BeginDate` (se puede indicar as\u00ed utilizando el operador . (punto) en el nombre de columna para `F.col(...)`), y hacer lo mismo con la columna `EndDate` y con la columna `Artist`.\n* El ejercicio completo se resuelve con solamente cinco transformaciones (y por tanto, cinco l\u00edneas de c\u00f3digo) que deben estar todas encadenadas.\n* Guardar el resultado de las cinco transformaciones anteriores en la variable `obras_limpias_df` y **cachearla** puesto que a partir de ahora trabajaremos con este DF para extraer insights.\n* Guardar el n\u00famero de filas de `obras_limpias_df` en la variable `obras_autores`. Puedes comprobar que ha aumentado respecto a los ejercicios anteriores ya que hemos explotado las obras que ten\u00edan m\u00faltiples autores y se han convertido en filas independientes."}, {"cell_type": "code", "execution_count": 8, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "018e3b18454becf17af29b2f4dfd3448", "grade": false, "grade_id": "explotado", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "\nobras_limpias_df = obras_preprocesado_df\\\n.withColumn(\"tripletas\", F.arrays_zip(F.col(\"Artist\"), F.col(\"BeginDate\"), F.col(\"EndDate\"))) \\\n    .withColumn(\"tripletas\", F.explode(\"tripletas\")) \\\n    .withColumn(\"BeginDate\", F.col(\"tripletas.BeginDate\")) \\\n    .withColumn(\"EndDate\", F.col(\"tripletas.EndDate\")) \\\n    .withColumn(\"Artist\", F.col(\"tripletas.Artist\"))\n\nobras_limpias_df.cache()\n\nobras_autores = obras_limpias_df.count()"}, {"cell_type": "code", "execution_count": 9, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "6ae0888cb8a9dfb17983e08b22cb84f3", "grade": true, "grade_id": "explotado_test", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(obras_limpias_df.count() == 139202)\n\ntipos = dict(obras_limpias_df.dtypes)\n# Comprobamos los tipos de datos de las columnas resultantes tras explotar la columna tripletas original\nassert(tipos[\"BeginDate\"] == \"int\")\nassert(tipos[\"EndDate\"] == \"int\")\nassert(tipos[\"Artist\"] == \"string\")\nassert(tipos[\"tripletas\"] == \"struct<Artist:string,BeginDate:int,EndDate:int>\")\n"}, {"cell_type": "markdown", "metadata": {}, "source": "**(1 punto)** Ejercicio 5\n\nPartiendo del DF `obras_limpias_df` construido y cacheado en el ejercicio anterior, se pide:\n\n* Reemplazar la columna `Date` (fecha de creaci\u00f3n de la obra) por el resultado de *extraer la primera ocurrencia de 4 d\u00edgitos seguidos de la columna Date* (el a\u00f1o de la obra) y convertirlo a tipo entero. Actualmente esa columna es un campo de texto libre donde generalmente se mencionan varios a\u00f1os (a veces es el a\u00f1o que empez\u00f3 esa obra, o bien el a\u00f1o que se dio a conocer, ...) , con lo que esta extracci\u00f3n no ser\u00e1 totalmente perfecta pero al menos nos dar\u00e1 un a\u00f1o de referencia. \n  * PISTA: utiliza dentro de `withColumn(...)` la funci\u00f3n `F.regexp_extract(nombreCol, \"(\\d\\d\\d\\d)\", 1)` para indicar que queremos extraer la *primera* ocurrencia de 4 d\u00edgitos seguidos, ya que la expresi\u00f3n regular `\\d` significa \"cualquier d\u00edgito\".\n  * PISTA: `regexp_extract(...)` devuelve un objeto de tipo Column sobre el que directamente podemos encadenar la llamada para convertirlo a columna de enteros.\n* Utilizando `F.when(...)`, crear una nueva columna `edad_autor` con: \n  * Si `BeginDate` es igual a 0, `edad_autor` debe ser igual a 41 (recuerda usar `F.lit(...)`), que es la mediana de la edad de las filas donde BeginDate s\u00ed tiene un valor positivo.\n  * En cualquier otro caso, la edad que ten\u00eda el autor cuando cre\u00f3 esa obra es la diferencia entre el a\u00f1o de creaci\u00f3n menos el a\u00f1o de nacimiento.\n* Crear una nueva columna `mediana_edad` que contenga, **para cada autor**, la mediana de la edad con la que cre\u00f3 sus obras (columna `edad_autor`). Dicho valor ser\u00e1 igual para todas las filas de un mismo autor, pero distinto para diferentes autores. \n  * PISTA: deben utilizarse *funciones de ventana* particionadas por el nombre del autor (guardar la ventana creada en la variable `ventana_autor`). **No est\u00e1 permitido utilizar JOIN**.\n  * PISTA: la mediana se calcula con la funci\u00f3n `percentile_approximate` que en Spark 2.4 todav\u00eda no existe en la API estructurada pero s\u00ed en SQL puro, por lo que la aplicaremos con `F.expr('percentile_approx(nombreCol, 0.5)')` ya que por definici\u00f3n, es el percentil 0.5.\n* Filtrar el DF resultante para quedarnos s\u00f3lo con aquellas filas en las que la edad del autor sea estrictamente positiva.\n* Guardar el resultado de estas cuatro transformaciones en la variable `edades_df`."}, {"cell_type": "code", "execution_count": 10, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "847f77b99112b1dbfd9cb0a64c57e7e3", "grade": false, "grade_id": "edades", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.sql.window import Window\n\nedades_df = obras_limpias_df.withColumn(\"Date\", F.regexp_extract(\"Date\", \"(\\d\\d\\d\\d)\", 1).cast(\"integer\")) \\\n    .withColumn(\"edad_autor\", F.when(F.col(\"BeginDate\") == 0, 41).otherwise(F.col(\"Date\") - F.col(\"BeginDate\"))) \\\n    .withColumn(\"mediana_edad\", F.expr('percentile_approx(edad_autor, 0.5)').over(Window.partitionBy(\"Artist\"))) \\\n    .filter(F.col(\"edad_autor\") > 0)\n\nventana_autor = Window.partitionBy(\"Artist\")"}, {"cell_type": "code", "execution_count": 11, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "d021094e98a18a4ae6ee65a7535d279d", "grade": true, "grade_id": "edades_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "cc = edades_df.columns\nassert(\"Date\" in cc and \"edad_autor\" in cc and \"mediana_edad\" in cc)\nassert(round(edades_df.select(F.mean(\"edad_autor\").alias(\"edad_media\")).first().edad_media, 2) == 43.53)\nassert(round(edades_df.select(F.mean(\"Date\").alias(\"date_media\")).first().date_media, 2) == 1955.92)\nassert(round(edades_df.select(F.mean(\"mediana_edad\").alias(\"mediana_media\")).first().mediana_media, 1) == 43.1)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "a7e41c43d1814fc26f99cebee61e8e00", "grade": false, "grade_id": "cell-23736708508609f6", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(1 punto)** Ejercicio 6\n\nA continuaci\u00f3n: \n\n* Crear en la variable `bucketizer_nacimiento` un objeto Bucketizer para discretizar la variable `BeginDate` con los puntos de corte `[-float(\"inf\"), 1900, 1920, 1940, 1960, 1980, 2000, 2022]`, estableciendo como columna de salida una nueva columna llamada `decada_nacimiento`. Existen obras y autores anteriores a 1900, pero no los tendremos en cuenta.\n* Crear en la variable `bucketizer_edad` otro objeto Bucketizer para discretizar la variable `edad_autor` en d\u00e9cadas con los puntos de corte [0, 20, 30, 40, 50, 60, float(\"inf\")], estableciendo como columna de salida una nueva columna llamada `decada_creacion`.\n* Crear en la variable `pipeline_bucketizers` un pipeline con ambos bucketizers. El pipeline debe contener **exclusivamente** estas dos etapas, y **ninguna m\u00e1s**, ni tampoco ning\u00fan algoritmo predictivo ni nada parecido.\n* Entrenar el pipeline con el DF `edades_df` y a continuaci\u00f3n transformar dicho dataframe, guardando el resultado en la variable `edades_discretizadas_df`.\n  * No es necesario hacer (ser\u00e1 penalizado) ninguna divisi\u00f3n en entrenamiento y test, puesto que no estamos entrenando ning\u00fan modelo predictivo."}, {"cell_type": "code", "execution_count": 12, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "36efa989c897ff7f3ea8c1e81a14181b", "grade": false, "grade_id": "buckets", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "from pyspark.ml.feature import Bucketizer\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import VectorAssembler\n\nbucketizer_nacimiento = Bucketizer(splits=[-float(\"inf\"), 1900, 1920, 1940, 1960, 1980, 2000, 2022],\n                                   inputCol=\"BeginDate\", outputCol=\"decada_nacimiento\")\nbucketizer_edad = Bucketizer(splits=[0, 20, 30, 40, 50, 60, float(\"inf\")],\n                             inputCol=\"edad_autor\", outputCol=\"decada_creacion\")\npipeline_bucketizers = Pipeline(stages=[bucketizer_nacimiento, bucketizer_edad])\n\nedades_discretizadas_df = pipeline_bucketizers.fit(edades_df).transform(edades_df)\n\n"}, {"cell_type": "code", "execution_count": 13, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "af86cb35a548e7102a5fd565ba8dcbe4", "grade": true, "grade_id": "buckets_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "from pyspark.ml.feature import Bucketizer\nassert(isinstance(bucketizer_nacimiento, Bucketizer))\nassert(isinstance(bucketizer_edad, Bucketizer))\nassert(bucketizer_nacimiento.getSplits() == [-float(\"inf\"), 1900, 1920, 1940, 1960, 1980, 2000, 2022] and\n       bucketizer_nacimiento.getInputCol() == \"BeginDate\" and\n       bucketizer_nacimiento.getOutputCol() == \"decada_nacimiento\")\n\nassert(bucketizer_edad.getSplits() == [0, 20, 30, 40, 50, 60, float(\"inf\")] and\n       bucketizer_edad.getInputCol() == \"edad_autor\" and\n       bucketizer_edad.getOutputCol() == \"decada_creacion\")\n\ntipos = dict(edades_discretizadas_df.dtypes)\nassert(\"decada_nacimiento\" in edades_discretizadas_df.columns and \"decada_creacion\" in edades_discretizadas_df.columns)\nassert(tipos[\"decada_nacimiento\"] == \"double\" and tipos[\"decada_creacion\"] == \"double\")"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "fab2d6c3f537e46f80ae7276c397f564", "grade": false, "grade_id": "cell-dc10eb3302284c7d", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**(2 puntos)** Ejercicio 7\n\nPartiendo de `edades_discretizadas_df` se pide:\n\n* Reemplazar la columna `decada_creacion` por el resultado de recategorizar sus valores a string de esta forma: \n0.0 -> `\"[0, 20)\"`, 1.0 -> `\"[20, 30)\"`, 2.0 -> `\"[30, 40)\"`, 3.0 -> `\"[40, 50)\"`, 4.0 -> `\"[50, 60)\"`, 5.0 -> `\"[60+\"`.\nUna opci\u00f3n es utilizar la funci\u00f3n `F.when(...)`. Las etiquetas han de ser exactmaente estas o no se validar\u00e1 la soluci\u00f3n.\n* Reemplazar la columna `decada_nacimiento` por el resultado de recategorizar sus valores a string de esta forma: \n0.0 -> `0-1900\"`, 1.0 -> `\"1900-20\"`, 2.0 -> `\"1920-40\"`, 3.0 -> `\"1940-60\"`, 4.0 -> `\"1960-80\"`, 5.0 -> `\"1980-2000\"`, 6.0 -> `\"2000-22`.\n* Crear un nuevo DF `obras_decada_df` que tenga tantas filas como d\u00e9cadas de nacimiento existen (es decir, 7) y tantas columnas como d\u00e9cadas contemplamos en la vida de un artista (es decir, 6) m\u00e1s una (la d\u00e9cada de nacimiento del artista, que debe ser la primera columna de todas). En cada casilla debe contener **el recuento** del n\u00famero total de obras que han creado durante esa d\u00e9cada de su vida (correspondiente a la columna) los artistas que han nacido en la d\u00e9cada correspondiente a esa fila. El DF resultante debe estar ordenado de menor a mayor en base a la columna de la d\u00e9cada de nacimiento. Los valores nulos generados debido a combinaciones inexistentes deben rellenarse por 0.\n  * PISTA: utiliza `groupBy(...).pivot(...)` y tambi\u00e9n la funci\u00f3n `fillna(0)` despu\u00e9s de la ordenaci\u00f3n.\n* Crear una nueva columna llamada `obras_totales` que contenga, en cada fila, el n\u00famero total de obras que han sido creadas por artistas que nacieron en la d\u00e9cada indicada por la fila. PISTA: utiliza **aritm\u00e9tica de columnas**, es decir, una operaci\u00f3n aritm\u00e9tica con los seis objetos columna involucrados. No hay que agrupar nada.\n* Reemplazar una a una cada una de las columnas `[0, 20), [20, 30) ..., [60+\"` por su equivalente en porcentaje (esto es, dividiendo la columna entre la columna `obras_totales` y multiplicando por 100). El resultado debe estar redondeado a 2 d\u00edgitos decimales, para lo cual debes aplicar `F.round(objetoColumna, 2)` al objeto columna resultante.\n* Guardar el resultado de las transformaciones anteriores en la variable `porcentajes_decadas_df`."}, {"cell_type": "code", "execution_count": 14, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "74c6182ef330bdb6ada3625d232c7eab", "grade": false, "grade_id": "porcentajes", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# reemplazar la columna decada_creacion por el resultado de recategorizar sus valores a string \n# reemplazar la columna decada_nacimiento por el resultado de recategorizar sus valores a string \n\nedades_discretizadas_df = edades_discretizadas_df \\\n    .withColumn(\"decada_creacion\", F.when(F.col(\"decada_creacion\") == 0.0, \"[0, 20)\") \\\n                                   .when(F.col(\"decada_creacion\") == 1.0, \"[20, 30)\") \\\n                                   .when(F.col(\"decada_creacion\") == 2.0, \"[30, 40)\") \\\n                                   .when(F.col(\"decada_creacion\") == 3.0, \"[40, 50)\") \\\n                                   .when(F.col(\"decada_creacion\") == 4.0, \"[50, 60)\") \\\n                                   .when(F.col(\"decada_creacion\") == 5.0, \"[60+\")) \\\n    .withColumn(\"decada_nacimiento\", F.when(F.col(\"decada_nacimiento\") == 0.0, \"0-1900\") \\\n                                       .when(F.col(\"decada_nacimiento\") == 1.0, \"1900-20\") \\\n                                       .when(F.col(\"decada_nacimiento\") == 2.0, \"1920-40\") \\\n                                       .when(F.col(\"decada_nacimiento\") == 3.0, \"1940-60\") \\\n                                       .when(F.col(\"decada_nacimiento\") == 4.0, \"1960-80\") \\\n                                       .when(F.col(\"decada_nacimiento\") == 5.0, \"1980-2000\") \\\n                                       .when(F.col(\"decada_nacimiento\") == 6.0, \"2000-22\"))"}, {"cell_type": "code", "execution_count": 17, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "74c6182ef330bdb6ada3625d232c7eab", "grade": false, "grade_id": "porcentajes", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# Crear un nuevo DF obras_decada_df que tenga tantas filas como d\u00e9cadas de nacimiento existen\n\nobras_decada_df = edades_discretizadas_df.groupBy(\"decada_nacimiento\").pivot(\"decada_creacion\").count().fillna(0).orderBy(\"decada_nacimiento\")\n\n# Crear una nueva columna llamada obras_totales\n\nobras_decada_df = obras_decada_df.withColumn(\"obras_totales\", sum([F.col(col) for col in obras_decada_df.columns[1:]]))\n\n\n# Reemplazar una a una cada una de las columnas por su equivalente en porcentaje\n# Guardar el resultado de las transformaciones anteriores en la variable porcentajes_decadas_df.\n\nfor decade in obras_decada_df.columns[1:]:\n    obras_decada_df = obras_decada_df.withColumn(decade, F.round((F.col(decade) / F.col(\"obras_totales\")) * 100, 2))\n    \nporcentajes_decada_df = obras_decada_df  "}, {"cell_type": "code", "execution_count": 20, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "41594f357b9c6dfe83b5826d384c0b46", "grade": true, "grade_id": "porcentajes_test", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "lista = porcentajes_decada_df.collect()\nassert(len(porcentajes_decada_df.columns) == 8) # debe tener 8 columnas que son las 6 de la d\u00e9cada de la vida, m\u00e1s la decada_nacimeinto m\u00e1s obras_totales\nassert(len(lista) == 7)               # el DF debe tener 7 filas porque hay 7 posibles categor\u00edas en decada_nacimiento\nassert(lista[0].decada_nacimiento == \"0-1900\"  and round(lista[0][\"[50, 60)\"]) == 15)\nassert(lista[1].decada_nacimiento == \"1900-20\" and round(lista[1][\"[30, 40)\"]) == 21)\nassert(lista[2].decada_nacimiento == \"1920-40\" and round(lista[2][\"[60+\"]) == 8)\nassert(lista[3].decada_nacimiento == \"1940-60\" and round(lista[3][\"[20, 30)\"]) == 23)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}