{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Operaciones avanzadas con DataFrames"}, {"cell_type": "markdown", "metadata": {}, "source": "## Descripci\u00f3n de las variables"}, {"cell_type": "markdown", "metadata": {}, "source": "El dataset, obtenido de <a target = \"_blank\" href=\"https://www.transtats.bts.gov/Fields.asp?Table_ID=236\">este link</a> est\u00e1 compuesto por las siguientes variables referidas siempre al a\u00f1o 2018:\n\n1. **Month** 1-4\n2. **DayofMonth** 1-31\n3. **DayOfWeek** 1 (Monday) - 7 (Sunday)\n4. **FlightDate** fecha del vuelo\n5. **Origin** c\u00f3digo IATA del aeropuerto de origen\n6. **OriginCity** ciudad donde est\u00e1 el aeropuerto de origen\n7. **Dest** c\u00f3digo IATA del aeropuerto de destino\n8. **DestCity** ciudad donde est\u00e1 el aeropuerto de destino  \n9. **DepTime** hora real de salida (local, hhmm)\n10. **DepDelay** retraso a la salida, en minutos\n11. **ArrTime** hora real de llegada (local, hhmm)\n12. **ArrDelay** retraso a la llegada, en minutos: se considera que un vuelo ha llegado \"on time\" si aterriz\u00f3 menos de 15 minutos m\u00e1s tarde de la hora prevista en el Computerized Reservations Systems (CRS).\n13. **Cancelled** si el vuelo fue cancelado (1 = s\u00ed, 0 = no)\n14. **CancellationCode** raz\u00f3n de cancelaci\u00f3n (A = aparato, B = tiempo atmosf\u00e9rico, C = NAS, D = seguridad)\n15. **Diverted** si el vuelo ha sido desviado (1 = s\u00ed, 0 = no)\n16. **ActualElapsedTime** tiempo real invertido en el vuelo\n17. **AirTime** en minutos\n18. **Distance** en millas\n19. **CarrierDelay** en minutos: El retraso del transportista est\u00e1 bajo el control del transportista a\u00e9reo. Ejemplos de sucesos que pueden determinar el retraso del transportista son: limpieza de la aeronave, da\u00f1o de la aeronave, espera de la llegada de los pasajeros o la tripulaci\u00f3n de conexi\u00f3n, equipaje, impacto de un p\u00e1jaro, carga de equipaje, servicio de comidas, computadora, equipo del transportista, problemas legales de la tripulaci\u00f3n (descanso del piloto o acompa\u00f1ante) , da\u00f1os por mercanc\u00edas peligrosas, inspecci\u00f3n de ingenier\u00eda, abastecimiento de combustible, pasajeros discapacitados, tripulaci\u00f3n retrasada, servicio de inodoros, mantenimiento, ventas excesivas, servicio de agua potable, denegaci\u00f3n de viaje a pasajeros en mal estado, proceso de embarque muy lento, equipaje de mano no v\u00e1lido, retrasos de peso y equilibrio.\n20. **WeatherDelay** en minutos: causado por condiciones atmosf\u00e9ricas extremas o peligrosas, previstas o que se han manifestado antes del despegue, durante el viaje, o a la llegada.\n21. **NASDelay** en minutos: retraso causado por el National Airspace System (NAS) por motivos como condiciones meteorol\u00f3gicas (perjudiciales pero no extremas), operaciones del aeropuerto, mucho tr\u00e1fico a\u00e9reo, problemas con los controladores a\u00e9reos, etc.\n22. **SecurityDelay** en minutos: causado por la evacuaci\u00f3n de una terminal, re-embarque de un avi\u00f3n debido a brechas en la seguridad, fallos en dispositivos del control de seguridad, colas demasiado largas en el control de seguridad, etc.\n23. **LateAircraftDelay** en minutos: debido al propio retraso del avi\u00f3n al llegar, problemas para conseguir aterrizar en un aeropuerto a una hora m\u00e1s tard\u00eda de la que estaba prevista."}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "import pyspark.sql.functions as F\nfrom pyspark.sql.types import IntegerType\n\n# Leemos los datos y quitamos filas con NA y convertimos a num\u00e9ricas las columnas inferidas incorrectamente\nflightsDF = spark.read\\\n                 .option(\"header\", \"true\")\\\n                 .option(\"inferSchema\", \"true\")\\\n                 .csv(\"gs://ucmbucket2023/datos/flights-jan-apr-2018.csv\")\n\n# Convertimos a enteros y re-categorizamos ArrDelay en una nueva columna ArrDelayCat\n# None (< 15 min), Slight(entre 15 y 60 min), Huge (> 60 min)\n\ncleanFlightsDF = flightsDF.withColumn(\"ArrDelayCat\", F.when(F.col(\"ArrDelay\") < 15, \"None\")\\\n                                                      .when((F.col(\"ArrDelay\") >= 15) & (F.col(\"ArrDelay\") < 60), \"Slight\")\\\n                                                      .otherwise(\"Huge\"))\\\n                           .cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "## Hagamos algunas preguntas a los datos para obtener conclusiones"}, {"cell_type": "markdown", "metadata": {}, "source": "Imaginemos que somos los due\u00f1os de una web de viajes que rastrea internet en busca de vuelos en agencias y otras p\u00e1ginas, los compara y recomienda el m\u00e1s adecuado para el aeropuerto. Junto con esta recomendaci\u00f3n, querr\u00edamos dar tambi\u00e9n informaci\u00f3n sobre vuelos fiables y no fiables en lo que respecta a la puntualidad. Esto depende de muchos factores, como el origen y destino, duraci\u00f3n del vuelo, hora del d\u00eda, etc."}, {"cell_type": "markdown", "metadata": {}, "source": "### Agrupaci\u00f3n y agregaciones"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<p><b>PREGUNTA</b>: \u00bfCu\u00e1les son los vuelos (origen, destino) con mayor retraso medio? \u00bfCu\u00e1ntos vuelos existen entre cada par de aeropuertos?</p>  Guardar el resultado en la variable averageDelayOriginDest\n<p><b>PISTA</b>: Tras hacer las agregaciones para cada pareja \"Origin\", \"Dest\" (una agregaci\u00f3n para el retraso medio y otra para contar), aplica el m\u00e9todo sort(F.col(\"avgDelay\").desc()) para ordenar de forma decreciente por la nueva columna del retraso medio.\n</div>\n\n**Nota:** vamos a practicar con la funci\u00f3n `F.round(columna, cifras_decimales)` que recibe un objeto columna num\u00e9rica y el n\u00famero de cifras decimales a las que queremos redondearlo, y devuelve otro objeto columna num\u00e9rico redondeado. OJO: el nuevo objeto columna devuelto no conserva el nombre que ten\u00eda el objeto original, sino que trae el nombre por defecto \"round(col_original, n_cifras)\" as\u00ed que conviene renombrarlo con `alias()`."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n    <p><b>PREGUNTA</b>: \u00bfEs el avi\u00f3n un medio de transporte fiable?</p>\n    <p>(a) Mostrar, para cada aeropuerto de destino, el n\u00famero de vuelos que hay en cada <i>categor\u00eda de retraso</i> (variable ArrDelayCat). En lugar de llamar agg(F.count(\"*\")), podemos llamar a la transformaci\u00f3n count() sobre el resultado de groupBy(), y crear\u00e1 autom\u00e1ticamente una columna llamada \"count\" con los conteos para cada grupo. Guardarlo en la variable arr_delay_cat_df.\n<p> (b) Ahora agrupa por cada aeropuerto de origen y de destino, y mostrando una columna distinta por cada tipo de retraso, con el recuento del n\u00famero de vuelos en cada combinaci\u00f3n. Guardarlo en la variable arr_delay_cat_pivot_df. PISTA: utilizar la funci\u00f3n pivot(\"colName\").</p>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<p><b>PREGUNTA</b>: \u00bfHay relaci\u00f3n entre el d\u00eda de la semana y el retraso a la salida o a la llegada?</p>\n    <p> (a) Sin usar la funci\u00f3n pivot, calcula el retraso medio a la salida y a la llegada para cada d\u00eda de la semana y ordena por una de ellas descendentemente.</p>\n    <p> (b) Ahora haz lo mismo para cada d\u00eda pero calculando solamente el retraso medio a la llegada, desagregado por cada aeropuerto de origen y destino, utilizando la funci\u00f3n pivot() para generar un DF con tantas columnas como d\u00edas de la semana, m\u00e1s dos (el origen y destino). </p>\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<p><b>LA FUNCI\u00d3N PIVOT</b>: Puede ser interesante ver, para cada (Origin, Dest), el retraso promedio por\nd\u00eda de la semana. Si agrupamos por esas tres variables (Origin, Dest, DayOfWeek), nuestro resultado tendr\u00eda demasiadas filas para ser f\u00e1cil de visualizar (7 x 1009 ya que hay 1009 combinaciones de (Origin, DayOfWeek)). En cambio, vamos a crear 7 columnas, una por d\u00eda de la semana, en nuestro resultado DF. Lo haremos utilizando una de las variables de agrupaci\u00f3n (DayOfWeek) como <i> variable pivot</i>. Como esta variable tiene 7 valores distintos, se crear\u00e1n 7 columnas nuevas. De esta manera, visualizaremos toda la informaci\u00f3n de cada combinaci\u00f3n (Origen, Dest) condensada en una fila con 7 columnas con los 7 retrasos promedio correspondientes a ese (Origen, Dest) en cada d\u00eda de la semana.\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Operaciones JOIN y de ventana"}, {"cell_type": "markdown", "metadata": {}, "source": "Estar\u00eda bien tener el retraso promedio de una ruta junto a cada vuelo, para que podamos ver qu\u00e9 vuelos tuvieron un retraso que fue superior o inferior al retraso promedio de esa ruta."}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n    <b> PREGUNTA </b>:\nUsa el averageDelayOriginDestDF creado anteriormente, elimina la columna de conteo y luego \u00fanerlo con cleanFlightsDF, utilizando Origin y Dest como columnas de enlace. Finalmente, selecciona solo las columnas Origin, Dest, DayOfWeek, ArrDelay y avgDelay del resultado.\n</div>\n\n**PREGUNTA**: \u00bfqu\u00e9 tipo de JOIN utilizar\u00edas? \u00bfEs relevante en este caso?"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n    <p><b>BONUS (OPCIONAL)</b>: crear una nueva columna <i>belowAverage</i> que tenga valor True si ArrDelay es menor que el avgDelay de esa ruta, y False en caso contrario. No utilizar la funci\u00f3n when() sino el operador de comparaci\u00f3n directamente entre columnas, la cual devolver\u00e1 una columna booleana.\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n    <b> PREGUNTA </b>:repetir la operaci\u00f3n utilizando funciones de ventana, sin usar `join`.\n</div>"}, {"cell_type": "markdown", "metadata": {}, "source": "### Funciones de agregaci\u00f3n para traer, en cada grupo, todos los valores de cierta columna que hay en ese grupo. Esto genera una columna de tipo lista o de tipo conjunto\n\n#### Funciones F.collect_list() y F.collect_set(), que pueden utilizarse dentro de agg() en groupBy().agg() y tambi\u00e9n en una ventana, como ocurre con cualquier funci\u00f3n de agregaci\u00f3n\n\nLa funci\u00f3n collect_list() devuelve una nueva columna de tipo lista que puede tener elementos repetidos. La funci\u00f3n collect_set devuelve una columna de tipo conjunto donde no hay repetidos.\n\n#### Funciones para manejar columnas de tipo vector: todas las que empiezan por array_ [aqu\u00ed](https://spark.apache.org/docs/3.1.3/api/python/reference/pyspark.sql.html#functions)"}, {"cell_type": "code", "execution_count": 24, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 74:===================>                                      (1 + 2) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n|Origin|Dest|fechas_sin_repetidos                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |primer_elemento|\n+------+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\n|ABE   |CLT |[2018-01-29, 2018-02-02, 2018-02-04, 2018-01-10, 2018-01-08, 2018-01-02, 2018-01-13, 2018-03-27, 2018-03-07, 2018-03-24, 2018-01-23, 2018-02-27, 2018-02-17, 2018-01-18, 2018-04-09, 2018-01-20, 2018-03-10, 2018-01-12, 2018-04-08, 2018-02-01, 2018-04-26, 2018-03-09, 2018-03-26, 2018-04-03, 2018-03-04, 2018-04-14, 2018-03-14, 2018-02-22, 2018-01-24, 2018-01-30, 2018-04-13, 2018-04-22, 2018-01-16, 2018-01-26, 2018-03-08, 2018-03-21, 2018-02-24, 2018-04-19, 2018-04-01, 2018-04-06, 2018-01-03, 2018-04-15, 2018-04-24, 2018-04-02, 2018-03-06, 2018-03-30, 2018-02-21, 2018-02-23, 2018-03-23, 2018-03-29, 2018-02-08, 2018-02-16, 2018-04-04, 2018-02-03, 2018-02-10, 2018-02-09, 2018-03-19, 2018-04-20, 2018-03-12, 2018-01-27, 2018-03-15, 2018-04-25, 2018-02-14, 2018-01-25, 2018-02-06, 2018-02-18, 2018-03-25, 2018-04-16, 2018-02-19, 2018-03-16, 2018-01-22, 2018-01-17, 2018-04-17, 2018-02-11, 2018-04-21, 2018-02-07, 2018-03-17, 2018-03-31, 2018-02-25, 2018-03-22, 2018-04-27, 2018-01-19, 2018-01-07, 2018-01-09, 2018-01-21, 2018-04-30, 2018-03-18, 2018-04-05, 2018-04-11, 2018-03-02, 2018-01-14, 2018-01-05, 2018-01-06, 2018-04-10, 2018-03-05, 2018-02-28, 2018-02-20, 2018-03-13, 2018-04-23, 2018-02-15, 2018-01-01, 2018-04-07, 2018-04-28, 2018-04-18, 2018-02-05, 2018-02-13, 2018-01-28, 2018-04-29, 2018-01-11, 2018-01-15, 2018-02-12, 2018-01-31, 2018-02-26, 2018-03-01, 2018-04-12, 2018-03-03, 2018-03-11, 2018-03-28, 2018-03-20, 2018-01-04]|2018-02-01     |\n|ABE   |FLL |[2018-01-06, 2018-01-27, 2018-02-28, 2018-04-25, 2018-02-14, 2018-01-10, 2018-04-14, 2018-03-14, 2018-04-07, 2018-01-13, 2018-04-28, 2018-01-24, 2018-04-18, 2018-03-21, 2018-03-07, 2018-03-24, 2018-02-24, 2018-01-17, 2018-01-03, 2018-04-21, 2018-02-07, 2018-03-17, 2018-03-31, 2018-02-17, 2018-01-31, 2018-02-21, 2018-01-20, 2018-03-03, 2018-03-10, 2018-03-28, 2018-04-04, 2018-04-11, 2018-02-03, 2018-02-10]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |2018-03-21     |\n|ABI   |DFW |[2018-01-29, 2018-02-02, 2018-02-04, 2018-01-10, 2018-01-08, 2018-01-02, 2018-01-13, 2018-03-27, 2018-03-07, 2018-03-24, 2018-01-23, 2018-02-27, 2018-02-17, 2018-01-18, 2018-04-09, 2018-01-20, 2018-03-10, 2018-01-12, 2018-04-08, 2018-02-01, 2018-04-26, 2018-03-09, 2018-03-26, 2018-04-03, 2018-03-04, 2018-04-14, 2018-03-14, 2018-02-22, 2018-01-24, 2018-01-30, 2018-04-13, 2018-04-22, 2018-01-16, 2018-01-26, 2018-03-08, 2018-03-21, 2018-02-24, 2018-04-19, 2018-04-01, 2018-04-06, 2018-01-03, 2018-04-15, 2018-04-24, 2018-04-02, 2018-03-06, 2018-03-30, 2018-02-21, 2018-02-23, 2018-03-23, 2018-03-29, 2018-02-08, 2018-02-16, 2018-04-04, 2018-02-03, 2018-02-10, 2018-02-09, 2018-03-19, 2018-04-20, 2018-03-12, 2018-01-27, 2018-03-15, 2018-04-25, 2018-02-14, 2018-01-25, 2018-02-06, 2018-02-18, 2018-03-25, 2018-04-16, 2018-02-19, 2018-03-16, 2018-01-22, 2018-01-17, 2018-04-17, 2018-02-11, 2018-04-21, 2018-02-07, 2018-03-17, 2018-03-31, 2018-02-25, 2018-03-22, 2018-04-27, 2018-01-19, 2018-01-07, 2018-01-09, 2018-01-21, 2018-04-30, 2018-03-18, 2018-04-05, 2018-04-11, 2018-03-02, 2018-01-14, 2018-01-05, 2018-01-06, 2018-04-10, 2018-03-05, 2018-02-28, 2018-02-20, 2018-03-13, 2018-04-23, 2018-02-15, 2018-01-01, 2018-04-07, 2018-04-28, 2018-04-18, 2018-02-05, 2018-02-13, 2018-01-28, 2018-04-29, 2018-01-11, 2018-01-15, 2018-02-12, 2018-01-31, 2018-02-26, 2018-03-01, 2018-04-12, 2018-03-03, 2018-03-11, 2018-03-28, 2018-03-20, 2018-01-04]|2018-02-01     |\n+------+----+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------+\nonly showing top 3 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "todasFechasDF = flightsDF.groupBy(\"Origin\", \"Dest\")\\\n                    .agg(F.collect_list(\"FlightDate\").alias(\"todas_fechas\"),\n                         F.collect_set(\"FlightDate\").alias(\"fechas_sin_repetidos\")\n                    )\\\n                    .withColumn(\"primer_elemento\", F.element_at(\"todas_fechas\", 1)) # primer elemento de cada array\n\n# mostramos 3 filas y solo algunas columnas para que no sea muy largo\ntodasFechasDF.select(\"Origin\", \"Dest\", \"fechas_sin_repetidos\", \"primer_elemento\").show(3, truncate=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Funciones para crear una columna de tipo estructura\n\nUna columna de tipo estructura representa un tipo de dato creado por el usuario, que puede ser \"plano\", es decir, ser simplemente una tupla, o puede ser \"jer\u00e1rquico\" porque dentro de la estructura existan campos que son, a su vez, otras estructuras. El caso m\u00e1s simple es crear una tupla con los valores de varias columnas, fila a fila. Es posible \"ordenar\" tuplas, porque ordena en base al primer elemento de cada tupla (el m\u00e1s a la izquierda), y si empatan entonces se fija en el segundo, y as\u00ed sucesivamente.\n\nEsto es \u00fatil cuando queremos ordenar dentro de un grupo en base a cierta columna, y despu\u00e9s queremos ver el valor concreto de otra columna diferente que estaba en la misma fila, emparejado con el valor m\u00e1ximo o m\u00ednimo (es decir, cuando buscamos m\u00e1ximo o m\u00ednimo pero *no queremos perder la correspondencia* de ese valor con otros valores de su fila).\n\nSe puede acceder a cada uno de los campos de una tupla con el operador `.` (punto)\n\n**Ejemplo**: para cada par de aeropuertos origen y destino, encontrar la fecha en la que tuvo lugar el vuelo con m\u00e1s retraso a la llegada. Como estamos creando una columna de tipo estructura con dos campos, de los cuales el primero es el ArrDelay, al buscar dentro de cada grupo el m\u00e1ximo de dicha columna de tipo estructura, se fijar\u00e1 justamente en el ArrDelay."}, {"cell_type": "code", "execution_count": 25, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 77:======================================>                   (2 + 1) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------+----+--------------------+-----------------+\n|Origin|Dest|max_struct          |fecha_max_retraso|\n+------+----+--------------------+-----------------+\n|ABE   |CLT |{347.0, 2018-02-04} |2018-02-04       |\n|ABE   |FLL |{340.0, 2018-03-07} |2018-03-07       |\n|ABI   |DFW |{397.0, 2018-01-01} |2018-01-01       |\n|ABQ   |MCO |{6.0, 2018-04-14}   |2018-04-14       |\n|ABQ   |ORD |{460.0, 2018-04-16} |2018-04-16       |\n|ABY   |ATL |{641.0, 2018-01-07} |2018-01-07       |\n|ACY   |FLL |{1385.0, 2018-04-05}|2018-04-05       |\n|ACY   |MCO |{296.0, 2018-04-27} |2018-04-27       |\n|ACY   |MYR |{352.0, 2018-04-21} |2018-04-21       |\n|ACY   |PBI |{132.0, 2018-02-10} |2018-02-10       |\n|ADQ   |ANC |{221.0, 2018-03-20} |2018-03-20       |\n|AEX   |DFW |{467.0, 2018-02-22} |2018-02-22       |\n|AGS   |ATL |{1366.0, 2018-01-23}|2018-01-23       |\n|AGS   |DFW |{80.0, 2018-04-03}  |2018-04-03       |\n|AGS   |PHL |{215.0, 2018-04-04} |2018-04-04       |\n|ALB   |CLT |{215.0, 2018-02-22} |2018-02-22       |\n|ALB   |DCA |{266.0, 2018-01-19} |2018-01-19       |\n|ALB   |EWR |{390.0, 2018-02-05} |2018-02-05       |\n|ALB   |IAD |{306.0, 2018-02-24} |2018-02-24       |\n|ALB   |MCO |{385.0, 2018-03-04} |2018-03-04       |\n+------+----+--------------------+-----------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import Window\n\nfechaMaxDelayDF = flightsDF\\\n                    .withColumn(\"estructura\", F.struct(\"ArrDelay\", \"FlightDate\"))\\\n                    .groupBy(\"Origin\", \"Dest\")\\\n                    .agg(F.max(\"estructura\").alias(\"max_struct\"))\\\n                    .withColumn(\"fecha_max_retraso\", F.col(\"max_struct.FlightDate\"))\n\nfechaMaxDelayDF.show(truncate=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-success\">\n<b> PREGUNTA </b>: Vamos a construir otro DF con informaci\u00f3n sobre los aeropuertos (en una situaci\u00f3n real, tendr\u00edamos otra tabla en la base de datos como la tabla de la entidad Aeropuerto). Sin embargo, solo tenemos informaci\u00f3n sobre algunos aeropuertos. Nos gustar\u00eda agregar esta informaci\u00f3n a cleanFlightsDF como nuevas columnas, teniendo en cuenta que queremos que la informaci\u00f3n del aeropuerto coincida con el aeropuerto de origen de flightsDF. Utilizar la operaci\u00f3n de uni\u00f3n adecuada para asegurarse de que no se perder\u00e1 ninguna de las filas existentes de cleanFlightsDF despu\u00e9s de la uni\u00f3n.\n</div>"}, {"cell_type": "code", "execution_count": 26, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 81:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+----+-------------------------------------+----+\n|IATA|FullName                             |Year|\n+----+-------------------------------------+----+\n|JFK |John F. Kennedy International Airport|1948|\n|LIT |Little Rock National Airport         |1931|\n|SEA |Seattle-Tacoma International Airport |1949|\n+----+-------------------------------------+----+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "airportsDF = spark.createDataFrame([\n    (\"JFK\", \"John F. Kennedy International Airport\", 1948),\n    (\"LIT\", \"Little Rock National Airport\", 1931),\n    (\"SEA\", \"Seattle-Tacoma International Airport\", 1949),\n], [\"IATA\", \"FullName\", \"Year\"])\n\nairportsDF.show(truncate=False)"}, {"cell_type": "markdown", "metadata": {}, "source": "## User-defined functions (UDFs)"}, {"cell_type": "markdown", "metadata": {}, "source": "Vamos a construir un UDF para convertir millas a kil\u00f3metros. Ten en cuenta que esto podr\u00eda hacerse f\u00e1cilmente multiplicando directamente la columna de millas por 1.6 (y ser\u00eda mucho m\u00e1s eficiente), ya que Spark permite el producto entre una columna y un n\u00famero. En todos los casos en los que Spark proporciona funciones integradas para realizar una tarea (como esta), debes usar esas funciones y no una UDF. Las UDF deben emplearse solo cuando no hay otra opci\u00f3n.\n\nLa raz\u00f3n es que las funciones integradas de Spark est\u00e1n optimizadas y Catalyst, el optimizador autom\u00e1tico de c\u00f3digo integrado en Spark, puede optimizarlo a\u00fan m\u00e1s. Sin embargo, las UDF son una caja negra para Catalyst y su contenido no se optimizar\u00e1, y por lo tanto, generalmente son mucho m\u00e1s lentas."}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "8.0\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 90:===================>                                      (1 + 2) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------+----+--------+------------------+\n|Origin|Dest|Distance|            DistKM|\n+------+----+--------+------------------+\n|   FAT| SFO|   158.0|             252.8|\n|   EWR| MYR|   550.0|             880.0|\n|   IAH| CLT|   912.0|            1459.2|\n|   BUF| EWR|   282.0|451.20000000000005|\n|   ROP| GUM|    56.0| 89.60000000000001|\n+------+----+--------+------------------+\nonly showing top 5 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import udf\nfrom pyspark.sql.types import DoubleType\n\n# Primer paso: crear una funci\u00f3n de Python que reciba UN n\u00famero y lo multiplique por 1.6\ndef milesToKm(miles):\n    return miles*1.6\n\n# Vamos a probarla\nprint(milesToKm(5)) # 5 millas a km: 8 km\n\n# Segundo paso: crear un objeto UDF que envuelva a nuestra funci\u00f3n. \n# Hay que especificar el tipo de dato que devuelve nuestra funci\u00f3n\nudfMilesToKm = F.udf(milesToKm, DoubleType())\n\n# Con esto, Spark ser\u00e1 capaz de llamar a nuestra funci\u00f3n milesToKm sobre cada uno de los valores de una columna num\u00e9rica.\n# Spark enviar\u00e1 el c\u00f3digo de nuestra funci\u00f3n a los executors a trav\u00e9s de la red, y cada executor la ejecutar\u00e1 sobre las\n# particiones (una por una) que est\u00e9n en ese executor\n\n# Tercer paso: vamos a probar la UDF a\u00f1adiendo una nueva columna con el resultado de la conversi\u00f3n\nflightsWithKm = cleanFlightsDF.withColumn(\"DistKm\", udfMilesToKm(F.col(\"Distance\")))\n\nflightsWithKm = cleanFlightsDF.withColumn(\"DistKm\", 1.6 * F.col(\"Distance\"))\n\n\nflightsWithKm.select(\"Origin\", \"Dest\", \"Distance\", \"DistKM\")\\\n             .distinct()\\\n             .show(5)"}, {"cell_type": "markdown", "metadata": {}, "source": "<div class=\"alert alert-block alert-info\">\n<p><b>BONUS</b>: Crea tu propia UDF que convierta DayOfWeek en una cadena.\nPuedes hacerlo creando una funci\u00f3n de Python que reciba un n\u00famero entero y devuelva el d\u00eda de la semana,\nsimplemente leyendo desde un vector de cadenas de longitud 7 el valor en la posici\u00f3n indicada por el argumento entero. Para la UDF, recuerda que tu funci\u00f3n devuelve un StringType(). Finalmente, prueba tu UDF creando una nueva columna \"DayOfWeekString\".\n</div>"}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 95:======================================>                   (2 + 1) / 3]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------+----+---------+---------------+\n|Origin|Dest|DayOfWeek|DayOfWeekString|\n+------+----+---------+---------------+\n|   SRQ| ORD|        3|      Wednesday|\n|   DEN| IAH|        2|        Tuesday|\n|   DCA| SFO|        2|        Tuesday|\n|   LAS| IAH|        2|        Tuesday|\n|   EWR| ATL|        1|         Monday|\n|   IAH| MCO|        1|         Monday|\n|   EWR| LAS|        1|         Monday|\n|   HNL| ORD|        1|         Monday|\n|   ORD| IAD|        7|         Sunday|\n|   LAX| DEN|        7|         Sunday|\n|   SJC| EWR|        7|         Sunday|\n|   DEN| OMA|        7|         Sunday|\n|   ROP| GUM|        1|         Monday|\n|   ROC| IAD|        1|         Monday|\n|   SAV| IAD|        1|         Monday|\n|   ORD| ROA|        7|         Sunday|\n|   IAD| ORD|        6|       Saturday|\n|   JAX| IAD|        6|       Saturday|\n|   ORD| PIA|        5|         Friday|\n|   DEN| BIS|        4|       Thursday|\n+------+----+---------+---------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.types import StringType\n\nfrom pyspark.sql import types as T\n\n# Primer paso: creamos una funci\u00f3n de python que convierte un n\u00famero entero en el d\u00eda de la semana como cadena\ndef dayOfWeekToString(dayInteger):\n    # En nuestros datos Monday es 1 pero las listas de python empiezan en el 0 y \n    # queremos usar el dayInteger como \u00edndice del vector\n    daysOfWeek = [\"\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n    #return ???\n    \n# Segundo paso: ajustamos nuestra funci\u00f3n con un Spark UDF para que Spark pueda invocarlo en cada valor de una columna completa\n# De esta manera, Spark puede enviar nuestra funci\u00f3n a los ejecutores, que eventualmente ejecutar\u00e1n la funci\u00f3n en las particiones\n# de los datos que tiene cada ejecutor\ndayOfWeekStringUDF = F.udf(dayOfWeekToString, T.StringType())\n\n# Tercer paso: intentemos nuestro UDF agregando una nueva columna que resulta de transformar (a trav\u00e9s del UDF) el\n# columna existente DayOfWeek\nflightsWithDayOfWeekStr = cleanFlightsDF.withColumn(\"DayOfWeekString\", dayOfWeekStringUDF(F.col(\"DayOfWeek\")))\n\nflightsWithDayOfWeekStr.select(\"Origin\", \"Dest\", \"DayOfWeek\", \"DayOfWeekString\")\\\n                       .distinct()\\\n                       .show()"}, {"cell_type": "markdown", "metadata": {}, "source": "## UDF con argumentos que no son columnas (usando currificaci\u00f3n)"}, {"cell_type": "code", "execution_count": 30, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---------------+\n|suma_o_producto|\n+---------------+\n|          130.0|\n|          288.0|\n|          264.0|\n|           80.0|\n|          105.0|\n|          264.0|\n|          351.0|\n|        24138.0|\n|          100.0|\n|          115.0|\n|            8.0|\n|         5544.0|\n|           40.0|\n|          420.0|\n|          490.0|\n|           55.0|\n|           60.0|\n|         3591.0|\n|           12.0|\n|          161.0|\n+---------------+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.types import DoubleType\n\ndef udf_currificada(param_config):\n    \"\"\"\n    El par\u00e1metro param_config configura si la funci\u00f3n se comporta como suma o como producto. \n    Este par\u00e1metro no proviene de una columna del DF sino que lo indicamos nosotros de antemano\n    \"\"\"\n    def funcion_interna(x1, x2):\n        if param_config == \"suma\":\n            return x1+x2\n        elif param_config == \"producto\":\n            return x1*x2\n\n    # Envolvemos como UDF el objeto funci\u00f3n junto al par\u00e1metro que hemos pasado. Aunque param_config\n    # es externo a la funci\u00f3n \"funcion_interna\", dicho par\u00e1metro es necesario para ejecutar esa funci\u00f3n y por\n    # tanto, forma parte de la \"clausura\" de la funci\u00f3n interna: es un \"dato adjunto\" de la funci\u00f3n interna\n    return F.udf(funcion_interna, DoubleType())\n\n\ndef funcion_interna(tipo, x1, x2):\n    if tipo == \"suma\":\n        return x1+x2\n    elif tipo == \"producto\":\n        return x1*x2\n\nudf_para_usar = udf_currificada(\"producto\")\n\ncleanFlightsDF.where(\"ArrDelay is not null and DepDelay is not null\")\\\n              .select(udf_currificada(\"producto\")(F.col(\"ArrDelay\"), F.col(\"DepDelay\")).alias(\"suma_o_producto\"))\\\n              .show()"}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}