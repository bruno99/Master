{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La colección del MoMA (Museo de Arte Moderno) de Nueva York\n",
    "\n",
    "### Disponible en Kaggle en:\n",
    "https://www.kaggle.com/momanyc/museum-collection\n",
    "\n",
    "(no obstante, debes usar exclusivamente el dataset que has recibido adjunto al email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Contexto\n",
    "\n",
    "El Museo de Arte Moderno (MoMA) adquirió sus primeras obras de arte en 1929, año de su fundación. Hoy, la colección en evolución del Museo contiene casi 200.000 obras de todo el mundo que abarcan los últimos 150 años. La colección incluye una gama cada vez mayor de expresión visual, que incluye pintura, escultura, grabado, dibujo, fotografía, arquitectura, diseño, cine y medios y artes escénicas.\n",
    "\n",
    "### Contenido\n",
    "\n",
    "MoMA se compromete a ayudar a todos a comprender, disfrutar y utilizar nuestra colección. El sitio web del Museo presenta 72.706 obras de arte de 20.956 artistas. El conjunto de datos de obras de arte contiene 130.262 registros, que representan todas las obras que se han incorporado a la colección del MoMA y están catalogadas en nuestra base de datos. Incluye metadatos básicos para cada obra, incluyendo título, artista, fecha, medio, dimensiones y fecha de adquisición por parte del Museo. Algunos de estos registros tienen información incompleta y se indican como \"no aprobados por el curador\". El conjunto de datos de artistas contiene 15.091 registros, que representan a todos los artistas que tienen obras en la colección del MoMA y han sido catalogados en nuestra base de datos. Incluye metadatos básicos para cada artista, incluido el nombre, la nacionalidad, el sexo, el año de nacimiento y el año de fallecimiento.\n",
    "\n",
    "\n",
    "### Variables y significado (sólo aquellas que se utilizarán)\n",
    "\n",
    "\n",
    "* Title: string - título de la obra\n",
    "* Artist: string - nombre del autor o autores\n",
    "* ConstituentID: string - identificador no usado\n",
    "* ArtistBio: string - biografía del autor\n",
    "* Nationality: string - nacionalidad del autor(es)\n",
    "* BeginDate: string - fecha(s) de nacimiento del autor(es)\n",
    "* EndDate: string - fecha(s) de fallecimiento del autor(es)\n",
    "* Gender: string - género del autor(es)\n",
    "* Date: string - fecha de creación de la obra\n",
    "* Medium: string - soporte físico en el que se creó la obra\n",
    "* Dimensions: string - dimensiones (todas)\n",
    "* CreditLine: string - forma de obtención de esa obra de arte por parte del museo\n",
    "* AccessionNumber: string - identificador no usado\n",
    "* Classification: string - tipología de la obra de arte\n",
    "* Department: string - departamento del museo al que pertenece\n",
    "* DateAcquired: string - fecha de adquisición\n",
    "* Cataloged: string - si está catalogada o no\n",
    "* ObjectID: string - identificador no usado\n",
    "* URL: string - enlace a la web del museo\n",
    "* ThumbnailURL: string - enlace a una imagen en miniatura\n",
    "* Circumference (cm): string - perímetro de la obra en cm\n",
    "* Depth (cm): string - profundidad en cm\n",
    "* Diameter (cm): string - diámetro en cm\n",
    "* Height (cm): string - altura en cm\n",
    "* Length (cm): string - longitud en cm\n",
    "* Weight (kg): string - peso de la obra en kg\n",
    "* Width (cm): string - anchura de la obra en cm\n",
    "* Seat Height (cm): string - altura del soporte donde se expone\n",
    "* Duration (sec.): string - duración de la obra en caso de ser audiovisual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nombre completo del alumno:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d664812d734466d986ae91e421e0c696",
     "grade": false,
     "grade_id": "cell-0afbc8d5ce9eb796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**INSTRUCCIONES**: en cada celda debes responder a la pregunta formulada, asegurándote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a None. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del cálculo quede guardado exactamente en la variable que venía inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). \n",
    "\n",
    "**No olvides borrar la línea raise NotImplementedError() de cada celda cuando hayas completado la solución de esa celda y quieras probarla.**\n",
    "\n",
    "Después de cada celda evaluable verás una celda con código. Ejecútala (no modifiques su código) y te dirá si tu solución es correcta o no. Además de esas pruebas, se realizarán algunas más (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la solución correcta o no. Asegúrate de que, al menos, todas las celdas indican que el código es correcto antes de enviar el notebook terminado.\n",
    "\n",
    "**Nunca se debe redondear ninguna cantidad si no lo pide explícitamente el enunciado**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d22d622bad7f7572f4c1d832d4a15f35",
     "grade": false,
     "grade_id": "cell-c101269ae661fc72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Sobre el dataset anterior (MoMA_Artworks.csv) se pide:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 punto)** Ejercicio 1\n",
    "\n",
    "* Leerlo tratando de que Spark infiera el tipo de dato de cada columna, y **cachearlo**. Debe guardarse en una variable llamada `artworks`.\n",
    "* Puesto que existen columnas que contienen una coma enmedio del valor, en esos casos los valores vienen entre comillas dobles. Spark ya contempla esta posibilidad y puede leerlas adecuadamente **si al leer le indicamos las siguientes opciones adicionales** además de las que ya sueles usar: `.option(\"quote\", \"\\\"\").option(\"escape\", \"\\\"\")`.\n",
    "* Asegúrate de que las **filas que no tienen el formato correcto sean descartadas**, indicando también la opción `mode` con el valor `DROPMALFORMED` como vimos en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebccd2b2f8af6e1228e1f9c503932f12",
     "grade": false,
     "grade_id": "read_csv",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# LÍNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n",
    "artworks = None # Sustituye None por las operaciones adecuadas\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cbe721b16a3c35e82b57ec6c28b04fbb",
     "grade": true,
     "grade_id": "read_csv_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(artworks.count() == 128234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f9fd60af913dde963f5500f9579b4c94",
     "grade": false,
     "grade_id": "cell-11065b99ddfcbb80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 punto)** Ejercicio 2\n",
    "\n",
    "Puesto que vamos a hacer cálculos relativos a las fechas de nacimiento y muerte de los autores de las obras, es necesario tenerlas como número. Actualmente están como años entre paréntesis, ej. (1924) o a veces como varios paréntesis, \n",
    "como \"(1924) (1931) (1918)\" cuando son varios autores. Vamos a limpiar esto para obtener en BeginDate y en EndDate una columna numérica.\n",
    "\n",
    "Partiendo del DF almacenado en la variable `artworks`, haciendo una única secuencia de transformaciones encadenadas, se pide:\n",
    "\n",
    "* Quedarse sólo con aquellas obras en las que `BeginDate` no es null y `EndDate` tampoco.\n",
    "* Sobre el DF resultante, reemplazar la columna `BeginDate` por el resultado de quitar los paréntesis. Consulta la documentación de la función `F.regexp_replace(nombreCol, stringBuscar, stringReemplazo)` donde el segundo argumento debe ser `\"\\(|\\)\"` que indica buscar tanto el ( como el ), y el tercer argumento debe ser la cadena vacía `\"\"` para que los elimine.\n",
    "* **Después de hacer esto**, la columna `BeginDate` debe ser reemplazada de nuevo por una columna de tipo vector, que es el resultado de invocar a `F.split(nombreCol, separador)` (función de `pyspark.sql.functions` que se aplica a una columna y devuelve otra columna de tipo vector tras haber cortado cada fila de la columna original por el carácter separador indicado). En nuestro caso el separador debe ser el carácter `\" \"` (un espacio en blanco).\n",
    "* Fíjate que estos dos pasos se pueden hacer en una sola línea, ya que es posible invocar a `F.split(F.regexp_replace(...))`.\n",
    "* Repetir los dos pasos anteriores en el mismo orden con la columna `EndDate`.\n",
    "* No te preocupes por las columnas que tengan una sola fecha: la función split las convertirá en vectores de un solo elemento.\n",
    "* Aplicar `F.split` para reemplazar la columna `Artist` por el resultado de cortar por el string  `\", \"` (es decir, la coma seguida de un espacio en blanco).\n",
    "* Por último, crear la coluna `n_autores` como la longitud de cada vector de la columna `Artist`, usando la función `F.size(nombreCol)` que se aplica a columnas de tipo colección.\n",
    "* El resultado debe guardarse en una variable llamada `obras_splitted_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fbb261c5b58380921e41d4040b5bfc3e",
     "grade": false,
     "grade_id": "split_fechas",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LÍNEA EVALUABLE. NO RENOMBRAR LAS VARIABLES\n",
    "obras_splitted_df = None   # Sustituye None por las operaciones adecuadas\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "410f3c6bbe735f9b0bab3b0e89245adc",
     "grade": true,
     "grade_id": "split_fechas_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(obras_splitted_df.count() == 126970)   # número de filas tras quitar las que tienen BeginDate y EndDate a null\n",
    "tipos = dict(obras_splitted_df.dtypes)\n",
    "assert(tipos[\"Artist\"] == \"array<string>\")  # la columna Artist ahora debe ser una columna de vectores de string\n",
    "assert(tipos[\"BeginDate\"] == \"array<string>\") # la columna BeginDate ahora debe ser una columna de vectores de string\n",
    "assert(tipos[\"EndDate\"] == \"array<string>\") # la columna EndDate debe ahora ser una columna de vectores de string\n",
    "assert(tipos[\"n_autores\"] == \"int\") # la columna EndDate debe ahora ser una columna de vectores de string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b76ca293cd3ecd7efbb0da34a401400a",
     "grade": false,
     "grade_id": "enunciado_todos_enteros",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 puntos)** Ejercicio 3\n",
    "\n",
    "En este ejercicio vamos a convertir todos los elementos de los vectores en números enteros. Además, a pesar de cómo hemos tratado de separar los elementos, existen algunas filas donde las columnas `BeginDate` y `EndDate` traen un texto que no son números entre paréntesis como nos gustaría, así que la separación no habrá funcionado muy bien. Vamos a quedarnos solo con aquellas filas que contienen exclusivamente enteros en las columnas.\n",
    "\n",
    "Partiendo de `obras_splitted_df` se pide:\n",
    "\n",
    "* Reemplazar la coluna `BeginDate` por el resultado de aplicar, en cada fila, una función definida por el usuario a **cada elemento del vector**. Existe en Pyspark una función llamada `transform` que hace justamente eso, pero en la versión 2.4 todavía no se puede invocar con la API de columnas sino que *existe solamente en la API de SQL puro* (esto es frecuente cuando los creadores de Spark introducen funciones nuevas: empieza estando disponible sólo para SQL puro y no para la API estructurada, que suele llegar en versiones posteriores).\n",
    "  * Para poder usarla, escribe `F.expr(\"transform(BeginDate, x -> int(x))\"))` como segundo argumento de `withColumn(...)`, donde la función que estamos aplicando a cada elemento del vector es simplemente transformarlo a entero.\n",
    "* Haz lo mismo con la columna `EndDate`.\n",
    "* A continuación, escribe una UDF que se aplique a una columna de tipo vector y compruebe si todos los elementos son enteros, en cuyo caso debe devolver `True`, y en caso contrario debe devolver `False`. Para ello:\n",
    "  * Rellena el esqueleto de la función de Python `todos_enteros` que tienes esbozada. El argumento que va a recibir siempre será una lista de Python por la que debes iterar, comprobando si cada elemento es de tipo entero. En el momento en el que encuentres uno que no es entero, ya puedes directamente retornar False. Si el bucle finaliza sin haber retornado en ninguna iteración, significa que todos los elementos son enteros, así que debe devolver True. Utiliza la función de Python `isinstance(x, int)` para comprobar si un elemento es entero.\n",
    "  * Crea en una variable `todos_enteros_udf` el objeto UDF que envuelve a la función anterior. Recuerda indicar que el valor devuelto por la función es BooleanType(), el cual debes importar adecuadamente para poder usarlo.\n",
    "\n",
    "Cuando tengas hecha la UDF:\n",
    "* Aplícala dentro de `withColumn(...)` para crear una nueva columna de booleanos llamada `enteros_begin` donde se indique si todos los elementos de cada fila de la columna `BeginDate` son enteros. Haz lo mismo para crear otra columna `enteros_end` que indique si todos los elementos de cada fila de la columna `EndDate` son enteros.\n",
    "* **Después de haber hecho lo anterior**, filtra las filas para quedarte solamente con aquellas en las que la longitud de cada vector de la columna `BeginDate` es igual a la longitud del vector de la columna `Artist` de esa misma fila, y además la columna `enteros_begin` es True en esa fila y además la columna `enteros_end` es True en esa fila (condición booleana compuesta por 3 condiciones simples).\n",
    "  * Consulta la documentación de la función `F.size(...)` que se aplica a una columna de tipo vector y devuelve una columna de enteros que son sus longitudes).\n",
    "* Primero debes escribir la UDF, y después una única secuencia de *cinco* transformaciones encadenadas que resuelva todo lo que pide este ejercicio. El resultado debe guardarse en la variable `obras_preprocesado_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "47961f67927f142c02de0b8e4c145f6e",
     "grade": false,
     "grade_id": "todos_enteros",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def todos_enteros(lista):\n",
    "    return None      # reemplaza esto por el código adecuado\n",
    "\n",
    "todos_enteros_udf = None   # reemplaza None por el código adecuado\n",
    "\n",
    "obras_preprocesado_df = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3a5dbe0d4cda723805b3cebcf48b074f",
     "grade": true,
     "grade_id": "todos_enteros_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(obras_preprocesado_df.count() == 126224)\n",
    "\n",
    "from pyspark.sql import types as T\n",
    "assert(todos_enteros([4, -123, 0, 23]) and not todos_enteros([23, 12, \"12\"]))  # probamos la función todos_enteros\n",
    "assert(todos_enteros_udf.returnType == T.BooleanType())  # la UDF debe devolver un tipo booleano\n",
    "\n",
    "r = obras_preprocesado_df.where(\"Title = 'House IV Project, Falls Village, Connecticut (Multiple axonometrics)'\")\\\n",
    ".select(\"Title\", \"Artist\", \"BeginDate\", \"EndDate\").first()\n",
    "assert(r.Artist == [\"Peter Eisenman\", \"Robert Cole\"] and r.BeginDate == [1932, 0] and r.EndDate == [0, 0])\n",
    "\n",
    "tipos = dict(obras_preprocesado_df.dtypes)\n",
    "assert(tipos[\"BeginDate\"] == \"array<int>\")\n",
    "assert(tipos[\"EndDate\"] == \"array<int>\")\n",
    "assert(tipos[\"enteros_begin\"] == \"boolean\")\n",
    "assert(tipos[\"enteros_end\"] == \"boolean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1c708deb5076dfee439d39d074e5e76",
     "grade": false,
     "grade_id": "cell-3cdceb86b7473ad3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 puntos)** Ejercicio 4\n",
    "\n",
    "Ya tenemos como vectores de enteros las columnas `BeginDate`, `EndDate` y `Artist`. Si todo ha ido bien, las longitudes de los vectores de esas tres columnas deben coincidir: si en determinada fila `BeginDate` contiene un vector de longitud 2 (por ejemplo), también deberían tener longitud 2 el vector de la columna `EndDate` de esa misma fila y el de la columna `Artist`de esa misma fila. \n",
    "\n",
    "Ahora queremos **explotar** esas columnas, es decir, si en `BeginDate` hay un vector de **n** elementos, es porque la obra tiene **n** autores distintos, pero nos gustaría que aparezca como **n filas distintas** en nuestro DF, en lugar de venir comprimido en una única fila con vectores. En cada una de esas filas separadas, nos gustaría ver a un autor concreto de la  obra, con su nombre, su fecha de nacimiento y de fallecimiento. Los valores del resto de columnas serán idénticos en esas **n** filas, y sólo difieren las columnas Artist, BeginDate y EndDate. \n",
    "\n",
    "¿Pero cuál de las tres columnas de tipo vector (`Artist, BeginDate, EndDate`) debemos usar para explotar? Utilizaremos *las tres conjuntamente*.\n",
    "\n",
    "Partiendo del DF `obras_preprocesado_df` construido en el ejercicio anterior, se pide:\n",
    "\n",
    "* Crear mediante `withColumn(...)` una nueva columna llamada `tripletas` que en cada fila tenga un **vector de estructuras**, fusionando para ello las columnas `Artist`, `BeginDate` y `EndDate`, invocando a la función `F.arrys_zip(nombreCol1, nombreCol2, nombreCol3)` que devuelve un objeto Column de tipo vector de estructuras (en nuestro caso, cada estructura será una tripleta con tres campos llamados `Artist`, `BeginDate`, `EndDate`). \n",
    "  * La función `F.arrays_zip` funciona adecuadamente porque los vectores que fusionamos tienen siempre el mismo tamaño entre sí (aunque el tamaño puede ser distinto en cada fila, pero coincide para esas columnas en cada fila concreta).\n",
    "  * Lo que hace la función es fusionar en una estructura elemento i-ésimo de cada uno de los vectores. La primera estructura estará formada por el primer elemento de Artist, de BeginDate y de EndDate; la segunda estructura será el segundo elemento de Artist, de BeginDate y de EndDate, y así sucesivamente. Todas estas estructuras se almacenan en un vector de estructuras en esa fila en la columna `tripletas`.\n",
    "* **Después de haber hecho esto**, se pide reemplazar la columna `tripletas` por el resultado de **explotar** dicha columna mediante la función `F.explode(nombreCol)`. El resultado será una columna de tripletas, donde ya no hay vectores puesto que cada vector de tripletas ha sido explotado, dando lugar a **varias filas independientes, en las cuales hay una tripleta distinta (Artist, BeginDate, EndDate) del vector explotado**. En el resto de columnas todas las filas son idénticas entre sí.\n",
    "* Por último, vamos a extraer como columnas cada uno de los campos de las estructuras. Se pide:\n",
    "  * Reemplazar la columna `BeginDate` por el campo `tripletas.BeginDate` (se puede indicar así utilizando el operador . (punto) en el nombre de columna para `F.col(...)`), y hacer lo mismo con la columna `EndDate` y con la columna `Artist`.\n",
    "* El ejercicio completo se resuelve con solamente cinco transformaciones (y por tanto, cinco líneas de código) que deben estar todas encadenadas.\n",
    "* Guardar el resultado de las cinco transformaciones anteriores en la variable `obras_limpias_df` y **cachearla** puesto que a partir de ahora trabajaremos con este DF para extraer insights.\n",
    "* Guardar el número de filas de `obras_limpias_df` en la variable `obras_autores`. Puedes comprobar que ha aumentado respecto a los ejercicios anteriores ya que hemos explotado las obras que tenían múltiples autores y se han convertido en filas independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "018e3b18454becf17af29b2f4dfd3448",
     "grade": false,
     "grade_id": "explotado",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "obras_limpias_df = None     # cambia None por el código necesario\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6ae0888cb8a9dfb17983e08b22cb84f3",
     "grade": true,
     "grade_id": "explotado_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(obras_limpias_df.count() == 139202)\n",
    "\n",
    "tipos = dict(obras_limpias_df.dtypes)\n",
    "# Comprobamos los tipos de datos de las columnas resultantes tras explotar la columna tripletas original\n",
    "assert(tipos[\"BeginDate\"] == \"int\")\n",
    "assert(tipos[\"EndDate\"] == \"int\")\n",
    "assert(tipos[\"Artist\"] == \"string\")\n",
    "assert(tipos[\"tripletas\"] == \"struct<Artist:string,BeginDate:int,EndDate:int>\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 punto)** Ejercicio 5\n",
    "\n",
    "Partiendo del DF `obras_limpias_df` construido y cacheado en el ejercicio anterior, se pide:\n",
    "\n",
    "* Reemplazar la columna `Date` (fecha de creación de la obra) por el resultado de *extraer la primera ocurrencia de 4 dígitos seguidos de la columna Date* (el año de la obra) y convertirlo a tipo entero. Actualmente esa columna es un campo de texto libre donde generalmente se mencionan varios años (a veces es el año que empezó esa obra, o bien el año que se dio a conocer, ...) , con lo que esta extracción no será totalmente perfecta pero al menos nos dará un año de referencia. \n",
    "  * PISTA: utiliza dentro de `withColumn(...)` la función `F.regexp_extract(nombreCol, \"(\\d\\d\\d\\d)\", 1)` para indicar que queremos extraer la *primera* ocurrencia de 4 dígitos seguidos, ya que la expresión regular `\\d` significa \"cualquier dígito\".\n",
    "  * PISTA: `regexp_extract(...)` devuelve un objeto de tipo Column sobre el que directamente podemos encadenar la llamada para convertirlo a columna de enteros.\n",
    "* Utilizando `F.when(...)`, crear una nueva columna `edad_autor` con: \n",
    "  * Si `BeginDate` es igual a 0, `edad_autor` debe ser igual a 41 (recuerda usar `F.lit(...)`), que es la mediana de la edad de las filas donde BeginDate sí tiene un valor positivo.\n",
    "  * En cualquier otro caso, la edad que tenía el autor cuando creó esa obra es la diferencia entre el año de creación menos el año de nacimiento.\n",
    "* Crear una nueva columna `mediana_edad` que contenga, **para cada autor**, la mediana de la edad con la que creó sus obras (columna `edad_autor`). Dicho valor será igual para todas las filas de un mismo autor, pero distinto para diferentes autores. \n",
    "  * PISTA: deben utilizarse *funciones de ventana* particionadas por el nombre del autor (guardar la ventana creada en la variable `ventana_autor`). **No está permitido utilizar JOIN**.\n",
    "  * PISTA: la mediana se calcula con la función `percentile_approximate` que en Spark 2.4 todavía no existe en la API estructurada pero sí en SQL puro, por lo que la aplicaremos con `F.expr('percentile_approx(nombreCol, 0.5)')` ya que por definición, es el percentil 0.5.\n",
    "* Filtrar el DF resultante para quedarnos sólo con aquellas filas en las que la edad del autor sea estrictamente positiva.\n",
    "* Guardar el resultado de estas cuatro transformaciones en la variable `edades_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "847f77b99112b1dbfd9cb0a64c57e7e3",
     "grade": false,
     "grade_id": "edades",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "edades_df = None    # reemplaza None por el código adecuado\n",
    "ventana_autor = None\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d021094e98a18a4ae6ee65a7535d279d",
     "grade": true,
     "grade_id": "edades_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cc = edades_df.columns\n",
    "assert(\"Date\" in cc and \"edad_autor\" in cc and \"mediana_edad\" in cc)\n",
    "assert(round(edades_df.select(F.mean(\"edad_autor\").alias(\"edad_media\")).first().edad_media, 2) == 43.53)\n",
    "assert(round(edades_df.select(F.mean(\"Date\").alias(\"date_media\")).first().date_media, 2) == 1955.92)\n",
    "assert(round(edades_df.select(F.mean(\"mediana_edad\").alias(\"mediana_media\")).first().mediana_media, 1) == 43.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7e41c43d1814fc26f99cebee61e8e00",
     "grade": false,
     "grade_id": "cell-23736708508609f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(1 punto)** Ejercicio 6\n",
    "\n",
    "A continuación: \n",
    "\n",
    "* Crear en la variable `bucketizer_nacimiento` un objeto Bucketizer para discretizar la variable `BeginDate` con los puntos de corte `[-float(\"inf\"), 1900, 1920, 1940, 1960, 1980, 2000, 2022]`, estableciendo como columna de salida una nueva columna llamada `decada_nacimiento`. Existen obras y autores anteriores a 1900, pero no los tendremos en cuenta.\n",
    "* Crear en la variable `bucketizer_edad` otro objeto Bucketizer para discretizar la variable `edad_autor` en décadas con los puntos de corte [0, 20, 30, 40, 50, 60, float(\"inf\")], estableciendo como columna de salida una nueva columna llamada `decada_creacion`.\n",
    "* Crear en la variable `pipeline_bucketizers` un pipeline con ambos bucketizers. El pipeline debe contener **exclusivamente** estas dos etapas, y **ninguna más**, ni tampoco ningún algoritmo predictivo ni nada parecido.\n",
    "* Entrenar el pipeline con el DF `edades_df` y a continuación transformar dicho dataframe, guardando el resultado en la variable `edades_discretizadas_df`.\n",
    "  * No es necesario hacer (será penalizado) ninguna división en entrenamiento y test, puesto que no estamos entrenando ningún modelo predictivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "36efa989c897ff7f3ea8c1e81a14181b",
     "grade": false,
     "grade_id": "buckets",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "bucketizer_nacimiento = None\n",
    "bucketizer_edad = None\n",
    "pipeline_bucketizers = None\n",
    "edades_discretizadas_df = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af86cb35a548e7102a5fd565ba8dcbe4",
     "grade": true,
     "grade_id": "buckets_test",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "assert(isinstance(bucketizer_nacimiento, Bucketizer))\n",
    "assert(isinstance(bucketizer_edad, Bucketizer))\n",
    "assert(bucketizer_nacimiento.getSplits() == [-float(\"inf\"), 1900, 1920, 1940, 1960, 1980, 2000, 2022] and\n",
    "       bucketizer_nacimiento.getInputCol() == \"BeginDate\" and\n",
    "       bucketizer_nacimiento.getOutputCol() == \"decada_nacimiento\")\n",
    "\n",
    "assert(bucketizer_edad.getSplits() == [0, 20, 30, 40, 50, 60, float(\"inf\")] and\n",
    "       bucketizer_edad.getInputCol() == \"edad_autor\" and\n",
    "       bucketizer_edad.getOutputCol() == \"decada_creacion\")\n",
    "\n",
    "tipos = dict(edades_discretizadas_df.dtypes)\n",
    "assert(\"decada_nacimiento\" in edades_discretizadas_df.columns and \"decada_creacion\" in edades_discretizadas_df.columns)\n",
    "assert(tipos[\"decada_nacimiento\"] == \"double\" and tipos[\"decada_creacion\"] == \"double\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fab2d6c3f537e46f80ae7276c397f564",
     "grade": false,
     "grade_id": "cell-dc10eb3302284c7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(2 puntos)** Ejercicio 7\n",
    "\n",
    "Partiendo de `edades_discretizadas_df` se pide:\n",
    "\n",
    "* Reemplazar la columna `decada_creacion` por el resultado de recategorizar sus valores a string de esta forma: \n",
    "0.0 -> `\"[0, 20)\"`, 1.0 -> `\"[20, 30)\"`, 2.0 -> `\"[30, 40)\"`, 3.0 -> `\"[40, 50)\"`, 4.0 -> `\"[50, 60)\"`, 5.0 -> `\"[60+\"`.\n",
    "Una opción es utilizar la función `F.when(...)`. Las etiquetas han de ser exactmaente estas o no se validará la solución.\n",
    "* Reemplazar la columna `decada_nacimiento` por el resultado de recategorizar sus valores a string de esta forma: \n",
    "0.0 -> `0-1900\"`, 1.0 -> `\"1900-20\"`, 2.0 -> `\"1920-40\"`, 3.0 -> `\"1940-60\"`, 4.0 -> `\"1960-80\"`, 5.0 -> `\"1980-2000\"`, 6.0 -> `\"2000-22`.\n",
    "* Crear un nuevo DF `obras_decada_df` que tenga tantas filas como décadas de nacimiento existen (es decir, 7) y tantas columnas como décadas contemplamos en la vida de un artista (es decir, 6) más una (la década de nacimiento del artista, que debe ser la primera columna de todas). En cada casilla debe contener **el recuento** del número total de obras que han creado durante esa década de su vida (correspondiente a la columna) los artistas que han nacido en la década correspondiente a esa fila. El DF resultante debe estar ordenado de menor a mayor en base a la columna de la década de nacimiento. Los valores nulos generados debido a combinaciones inexistentes deben rellenarse por 0.\n",
    "  * PISTA: utiliza `groupBy(...).pivot(...)` y también la función `fillna(0)` después de la ordenación.\n",
    "* Crear una nueva columna llamada `obras_totales` que contenga, en cada fila, el número total de obras que han sido creadas por artistas que nacieron en la década indicada por la fila. PISTA: utiliza **aritmética de columnas**, es decir, una operación aritmética con los seis objetos columna involucrados. No hay que agrupar nada.\n",
    "* Reemplazar una a una cada una de las columnas `[0, 20), [20, 30) ..., [60+\"` por su equivalente en porcentaje (esto es, dividiendo la columna entre la columna `obras_totales` y multiplicando por 100). El resultado debe estar redondeado a 2 dígitos decimales, para lo cual debes aplicar `F.round(objetoColumna, 2)` al objeto columna resultante.\n",
    "* Guardar el resultado de las transformaciones anteriores en la variable `porcentajes_decadas_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74c6182ef330bdb6ada3625d232c7eab",
     "grade": false,
     "grade_id": "porcentajes",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "porcentajes_decada_df = None     # reemplaza None por el valor adecuado\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41594f357b9c6dfe83b5826d384c0b46",
     "grade": true,
     "grade_id": "porcentajes_test",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lista = porcentajes_decada_df.collect()\n",
    "assert(len(porcentajes_decada_df.columns) == 8) # debe tener 8 columnas que son las 6 de la década de la vida, más la decada_nacimeinto más obras_totales\n",
    "assert(len(lista) == 7)               # el DF debe tener 7 filas porque hay 7 posibles categorías en decada_nacimiento\n",
    "assert(lista[0].decada_nacimiento == \"0-1900\"  and round(lista[0][\"[50, 60)\"]) == 15)\n",
    "assert(lista[1].decada_nacimiento == \"1900-20\" and round(lista[1][\"[30, 40)\"]) == 21)\n",
    "assert(lista[2].decada_nacimiento == \"1920-40\" and round(lista[2][\"[60+\"]) == 8)\n",
    "assert(lista[3].decada_nacimiento == \"1940-60\" and round(lista[3][\"[20, 30)\"]) == 23)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
